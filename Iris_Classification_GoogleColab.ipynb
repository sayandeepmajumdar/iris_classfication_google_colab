{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iris_Classification_GoogleColab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1eIN/j0TABE63j+qANfzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayandeepmajumdar/iris_classfication_google_colab/blob/master/Iris_Classification_GoogleColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0f2f3DbDxPa",
        "colab_type": "text"
      },
      "source": [
        "**Iris Classification**\n",
        "\n",
        "Import needed library in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGyaGt-FECTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from joblib import dump, load"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md7S3nWbF6D6",
        "colab_type": "text"
      },
      "source": [
        "Loading the Dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BifJpW9tGCfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "04e8dd11-42d9-4d1c-9713-682d626c71f7"
      },
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "new_names = ['sepal_length','sepal_width','petal_length','petal_width','iris_class']\n",
        "dataset = pd.read_csv(url, names=new_names, skiprows=0, delimiter=',')\n",
        "dataset.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            "sepal_length    150 non-null float64\n",
            "sepal_width     150 non-null float64\n",
            "petal_length    150 non-null float64\n",
            "petal_width     150 non-null float64\n",
            "iris_class      150 non-null object\n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8COXR6JFGJrx",
        "colab_type": "text"
      },
      "source": [
        "Explore and visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wo6VazEGKte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "9cd09bcb-7ec9-4150-c6b4-f7a130917bd6"
      },
      "source": [
        "dataset.describe()\n",
        "dataset.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width   iris_class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvsS2G3aGXW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3e7ef29e-32cf-44b5-d139-dd370985f709"
      },
      "source": [
        "y = dataset['iris_class']\n",
        "x = dataset.drop(['iris_class'], axis=1)\n",
        "\n",
        "print (\"dataset : \",dataset.shape)\n",
        "print (\"x : \",x.shape)\n",
        "print (\"y : \",y.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset :  (150, 5)\n",
            "x :  (150, 4)\n",
            "y :  (150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD3nAqb8JbpH",
        "colab_type": "text"
      },
      "source": [
        "we will use a Multi-Layer Perceptron (MLP) Classifier. We need to encode our target attribute for Neural Network based classifier into one hot format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htkpMh5LGbnz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "c5f8459a-333e-489c-a424-0923ab214a53"
      },
      "source": [
        "y=pd.get_dummies(y)\n",
        "y.sample(7)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "94             0                1               0\n",
              "47             1                0               0\n",
              "74             0                1               0\n",
              "80             0                1               0\n",
              "121            0                0               1\n",
              "45             1                0               0\n",
              "50             0                1               0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBy2a7s0GkyC",
        "colab_type": "text"
      },
      "source": [
        "Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QEESPPfGmVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4b61b0ef-cc00-4a5e-9613-af9702799765"
      },
      "source": [
        "\n",
        "#Selective import Scikit Learn \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate Training and Validation Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) #0.3 data as data test\n",
        "\n",
        "#converting to float 32bit\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "x_test  = np.array(x_test).astype(np.float32)\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "y_test  = np.array(y_test).astype(np.float32)\n",
        "\n",
        "#print data split for validation\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(105, 4) (105, 3)\n",
            "(45, 4) (45, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEIjkiggGy3q",
        "colab_type": "text"
      },
      "source": [
        "Select an algorithm and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnOjQ2O_Gz67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4d46cbf-b9c3-4eee-9e5a-9a1277ab3cd8"
      },
      "source": [
        "#Importing our model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#model initialization\n",
        "Model = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=1,  random_state=121) #try verbode=0 to train with out logging\n",
        "#train our model\n",
        "h=Model.fit(x_train,y_train)\n",
        "#use our model to predict\n",
        "y_pred=Model.predict(x_test)\n",
        "print(y_pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.54491592\n",
            "Iteration 2, loss = 2.50673771\n",
            "Iteration 3, loss = 2.45734456\n",
            "Iteration 4, loss = 2.40045797\n",
            "Iteration 5, loss = 2.34060884\n",
            "Iteration 6, loss = 2.28098455\n",
            "Iteration 7, loss = 2.22441802\n",
            "Iteration 8, loss = 2.17154553\n",
            "Iteration 9, loss = 2.12319162\n",
            "Iteration 10, loss = 2.07985162\n",
            "Iteration 11, loss = 2.04179189\n",
            "Iteration 12, loss = 2.00898545\n",
            "Iteration 13, loss = 1.98128490\n",
            "Iteration 14, loss = 1.95941650\n",
            "Iteration 15, loss = 1.94193152\n",
            "Iteration 16, loss = 1.92833723\n",
            "Iteration 17, loss = 1.91812835\n",
            "Iteration 18, loss = 1.91052674\n",
            "Iteration 19, loss = 1.90512404\n",
            "Iteration 20, loss = 1.90140618\n",
            "Iteration 21, loss = 1.89852366\n",
            "Iteration 22, loss = 1.89571225\n",
            "Iteration 23, loss = 1.89276517\n",
            "Iteration 24, loss = 1.88952978\n",
            "Iteration 25, loss = 1.88591745\n",
            "Iteration 26, loss = 1.88197663\n",
            "Iteration 27, loss = 1.87789633\n",
            "Iteration 28, loss = 1.87359714\n",
            "Iteration 29, loss = 1.86909081\n",
            "Iteration 30, loss = 1.86440430\n",
            "Iteration 31, loss = 1.85955362\n",
            "Iteration 32, loss = 1.85470028\n",
            "Iteration 33, loss = 1.84982464\n",
            "Iteration 34, loss = 1.84492806\n",
            "Iteration 35, loss = 1.84003867\n",
            "Iteration 36, loss = 1.83521412\n",
            "Iteration 37, loss = 1.83050079\n",
            "Iteration 38, loss = 1.82587999\n",
            "Iteration 39, loss = 1.82136624\n",
            "Iteration 40, loss = 1.81696029\n",
            "Iteration 41, loss = 1.81265863\n",
            "Iteration 42, loss = 1.80845044\n",
            "Iteration 43, loss = 1.80442954\n",
            "Iteration 44, loss = 1.80056477\n",
            "Iteration 45, loss = 1.79695267\n",
            "Iteration 46, loss = 1.79360567\n",
            "Iteration 47, loss = 1.79047410\n",
            "Iteration 48, loss = 1.78743046\n",
            "Iteration 49, loss = 1.78452878\n",
            "Iteration 50, loss = 1.78167483\n",
            "Iteration 51, loss = 1.77881487\n",
            "Iteration 52, loss = 1.77596550\n",
            "Iteration 53, loss = 1.77313480\n",
            "Iteration 54, loss = 1.77032904\n",
            "Iteration 55, loss = 1.76755259\n",
            "Iteration 56, loss = 1.76485900\n",
            "Iteration 57, loss = 1.76221478\n",
            "Iteration 58, loss = 1.75958845\n",
            "Iteration 59, loss = 1.75697392\n",
            "Iteration 60, loss = 1.75435626\n",
            "Iteration 61, loss = 1.75176080\n",
            "Iteration 62, loss = 1.74918593\n",
            "Iteration 63, loss = 1.74665905\n",
            "Iteration 64, loss = 1.74413981\n",
            "Iteration 65, loss = 1.74164008\n",
            "Iteration 66, loss = 1.73916059\n",
            "Iteration 67, loss = 1.73667986\n",
            "Iteration 68, loss = 1.73419582\n",
            "Iteration 69, loss = 1.73172257\n",
            "Iteration 70, loss = 1.72924443\n",
            "Iteration 71, loss = 1.72676003\n",
            "Iteration 72, loss = 1.72428252\n",
            "Iteration 73, loss = 1.72179472\n",
            "Iteration 74, loss = 1.71930168\n",
            "Iteration 75, loss = 1.71680580\n",
            "Iteration 76, loss = 1.71429656\n",
            "Iteration 77, loss = 1.71177336\n",
            "Iteration 78, loss = 1.70923536\n",
            "Iteration 79, loss = 1.70668132\n",
            "Iteration 80, loss = 1.70411003\n",
            "Iteration 81, loss = 1.70151966\n",
            "Iteration 82, loss = 1.69890885\n",
            "Iteration 83, loss = 1.69627630\n",
            "Iteration 84, loss = 1.69362057\n",
            "Iteration 85, loss = 1.69094014\n",
            "Iteration 86, loss = 1.68825570\n",
            "Iteration 87, loss = 1.68560693\n",
            "Iteration 88, loss = 1.68293842\n",
            "Iteration 89, loss = 1.68025028\n",
            "Iteration 90, loss = 1.67754247\n",
            "Iteration 91, loss = 1.67481452\n",
            "Iteration 92, loss = 1.67206576\n",
            "Iteration 93, loss = 1.66929544\n",
            "Iteration 94, loss = 1.66650264\n",
            "Iteration 95, loss = 1.66368637\n",
            "Iteration 96, loss = 1.66085154\n",
            "Iteration 97, loss = 1.65799958\n",
            "Iteration 98, loss = 1.65513652\n",
            "Iteration 99, loss = 1.65224949\n",
            "Iteration 100, loss = 1.64933762\n",
            "Iteration 101, loss = 1.64640156\n",
            "Iteration 102, loss = 1.64344135\n",
            "Iteration 103, loss = 1.64045655\n",
            "Iteration 104, loss = 1.63744656\n",
            "Iteration 105, loss = 1.63441067\n",
            "Iteration 106, loss = 1.63134807\n",
            "Iteration 107, loss = 1.62825794\n",
            "Iteration 108, loss = 1.62513944\n",
            "Iteration 109, loss = 1.62199177\n",
            "Iteration 110, loss = 1.61881378\n",
            "Iteration 111, loss = 1.61560334\n",
            "Iteration 112, loss = 1.61236131\n",
            "Iteration 113, loss = 1.60908731\n",
            "Iteration 114, loss = 1.60578089\n",
            "Iteration 115, loss = 1.60244168\n",
            "Iteration 116, loss = 1.59907150\n",
            "Iteration 117, loss = 1.59566877\n",
            "Iteration 118, loss = 1.59223243\n",
            "Iteration 119, loss = 1.58877643\n",
            "Iteration 120, loss = 1.58530938\n",
            "Iteration 121, loss = 1.58180945\n",
            "Iteration 122, loss = 1.57827711\n",
            "Iteration 123, loss = 1.57471355\n",
            "Iteration 124, loss = 1.57114119\n",
            "Iteration 125, loss = 1.56754007\n",
            "Iteration 126, loss = 1.56390910\n",
            "Iteration 127, loss = 1.56025061\n",
            "Iteration 128, loss = 1.55656893\n",
            "Iteration 129, loss = 1.55286708\n",
            "Iteration 130, loss = 1.54913996\n",
            "Iteration 131, loss = 1.54538687\n",
            "Iteration 132, loss = 1.54160850\n",
            "Iteration 133, loss = 1.53781128\n",
            "Iteration 134, loss = 1.53398010\n",
            "Iteration 135, loss = 1.53013866\n",
            "Iteration 136, loss = 1.52627063\n",
            "Iteration 137, loss = 1.52237682\n",
            "Iteration 138, loss = 1.51846095\n",
            "Iteration 139, loss = 1.51456833\n",
            "Iteration 140, loss = 1.51066399\n",
            "Iteration 141, loss = 1.50674416\n",
            "Iteration 142, loss = 1.50282832\n",
            "Iteration 143, loss = 1.49893324\n",
            "Iteration 144, loss = 1.49502442\n",
            "Iteration 145, loss = 1.49110265\n",
            "Iteration 146, loss = 1.48718097\n",
            "Iteration 147, loss = 1.48324388\n",
            "Iteration 148, loss = 1.47931581\n",
            "Iteration 149, loss = 1.47539687\n",
            "Iteration 150, loss = 1.47147641\n",
            "Iteration 151, loss = 1.46755138\n",
            "Iteration 152, loss = 1.46363443\n",
            "Iteration 153, loss = 1.45972216\n",
            "Iteration 154, loss = 1.45580169\n",
            "Iteration 155, loss = 1.45187034\n",
            "Iteration 156, loss = 1.44793737\n",
            "Iteration 157, loss = 1.44399362\n",
            "Iteration 158, loss = 1.44003942\n",
            "Iteration 159, loss = 1.43608633\n",
            "Iteration 160, loss = 1.43212327\n",
            "Iteration 161, loss = 1.42814994\n",
            "Iteration 162, loss = 1.42416748\n",
            "Iteration 163, loss = 1.42017587\n",
            "Iteration 164, loss = 1.41617651\n",
            "Iteration 165, loss = 1.41216712\n",
            "Iteration 166, loss = 1.40815430\n",
            "Iteration 167, loss = 1.40413317\n",
            "Iteration 168, loss = 1.40010711\n",
            "Iteration 169, loss = 1.39607244\n",
            "Iteration 170, loss = 1.39203210\n",
            "Iteration 171, loss = 1.38798622\n",
            "Iteration 172, loss = 1.38393626\n",
            "Iteration 173, loss = 1.37988234\n",
            "Iteration 174, loss = 1.37582473\n",
            "Iteration 175, loss = 1.37176335\n",
            "Iteration 176, loss = 1.36769859\n",
            "Iteration 177, loss = 1.36363100\n",
            "Iteration 178, loss = 1.35956130\n",
            "Iteration 179, loss = 1.35549007\n",
            "Iteration 180, loss = 1.35141742\n",
            "Iteration 181, loss = 1.34734446\n",
            "Iteration 182, loss = 1.34327101\n",
            "Iteration 183, loss = 1.33919874\n",
            "Iteration 184, loss = 1.33512664\n",
            "Iteration 185, loss = 1.33105619\n",
            "Iteration 186, loss = 1.32698922\n",
            "Iteration 187, loss = 1.32293286\n",
            "Iteration 188, loss = 1.31886390\n",
            "Iteration 189, loss = 1.31482032\n",
            "Iteration 190, loss = 1.31076326\n",
            "Iteration 191, loss = 1.30672041\n",
            "Iteration 192, loss = 1.30268706\n",
            "Iteration 193, loss = 1.29865536\n",
            "Iteration 194, loss = 1.29463094\n",
            "Iteration 195, loss = 1.29061748\n",
            "Iteration 196, loss = 1.28661175\n",
            "Iteration 197, loss = 1.28262153\n",
            "Iteration 198, loss = 1.27864264\n",
            "Iteration 199, loss = 1.27467213\n",
            "Iteration 200, loss = 1.27071084\n",
            "Iteration 201, loss = 1.26676273\n",
            "Iteration 202, loss = 1.26283237\n",
            "Iteration 203, loss = 1.25891336\n",
            "Iteration 204, loss = 1.25500725\n",
            "Iteration 205, loss = 1.25111425\n",
            "Iteration 206, loss = 1.24723511\n",
            "Iteration 207, loss = 1.24337042\n",
            "Iteration 208, loss = 1.23952068\n",
            "Iteration 209, loss = 1.23568607\n",
            "Iteration 210, loss = 1.23186726\n",
            "Iteration 211, loss = 1.22806410\n",
            "Iteration 212, loss = 1.22427661\n",
            "Iteration 213, loss = 1.22050490\n",
            "Iteration 214, loss = 1.21674906\n",
            "Iteration 215, loss = 1.21300946\n",
            "Iteration 216, loss = 1.20928652\n",
            "Iteration 217, loss = 1.20557928\n",
            "Iteration 218, loss = 1.20188889\n",
            "Iteration 219, loss = 1.19821527\n",
            "Iteration 220, loss = 1.19455980\n",
            "Iteration 221, loss = 1.19092800\n",
            "Iteration 222, loss = 1.18732300\n",
            "Iteration 223, loss = 1.18373048\n",
            "Iteration 224, loss = 1.18015693\n",
            "Iteration 225, loss = 1.17660385\n",
            "Iteration 226, loss = 1.17307304\n",
            "Iteration 227, loss = 1.16956525\n",
            "Iteration 228, loss = 1.16607832\n",
            "Iteration 229, loss = 1.16261308\n",
            "Iteration 230, loss = 1.15917025\n",
            "Iteration 231, loss = 1.15575036\n",
            "Iteration 232, loss = 1.15235280\n",
            "Iteration 233, loss = 1.14897789\n",
            "Iteration 234, loss = 1.14562524\n",
            "Iteration 235, loss = 1.14229470\n",
            "Iteration 236, loss = 1.13898679\n",
            "Iteration 237, loss = 1.13570057\n",
            "Iteration 238, loss = 1.13243654\n",
            "Iteration 239, loss = 1.12919398\n",
            "Iteration 240, loss = 1.12597404\n",
            "Iteration 241, loss = 1.12278609\n",
            "Iteration 242, loss = 1.11962057\n",
            "Iteration 243, loss = 1.11647793\n",
            "Iteration 244, loss = 1.11335871\n",
            "Iteration 245, loss = 1.11026344\n",
            "Iteration 246, loss = 1.10719244\n",
            "Iteration 247, loss = 1.10414583\n",
            "Iteration 248, loss = 1.10112389\n",
            "Iteration 249, loss = 1.09812629\n",
            "Iteration 250, loss = 1.09515294\n",
            "Iteration 251, loss = 1.09220333\n",
            "Iteration 252, loss = 1.08927772\n",
            "Iteration 253, loss = 1.08637496\n",
            "Iteration 254, loss = 1.08349546\n",
            "Iteration 255, loss = 1.08063884\n",
            "Iteration 256, loss = 1.07780434\n",
            "Iteration 257, loss = 1.07499321\n",
            "Iteration 258, loss = 1.07220344\n",
            "Iteration 259, loss = 1.06943655\n",
            "Iteration 260, loss = 1.06669139\n",
            "Iteration 261, loss = 1.06396898\n",
            "Iteration 262, loss = 1.06126794\n",
            "Iteration 263, loss = 1.05859004\n",
            "Iteration 264, loss = 1.05593314\n",
            "Iteration 265, loss = 1.05329785\n",
            "Iteration 266, loss = 1.05068503\n",
            "Iteration 267, loss = 1.04809291\n",
            "Iteration 268, loss = 1.04554450\n",
            "Iteration 269, loss = 1.04301265\n",
            "Iteration 270, loss = 1.04050293\n",
            "Iteration 271, loss = 1.03801470\n",
            "Iteration 272, loss = 1.03554866\n",
            "Iteration 273, loss = 1.03310526\n",
            "Iteration 274, loss = 1.03068464\n",
            "Iteration 275, loss = 1.02828664\n",
            "Iteration 276, loss = 1.02591191\n",
            "Iteration 277, loss = 1.02355776\n",
            "Iteration 278, loss = 1.02122610\n",
            "Iteration 279, loss = 1.01891436\n",
            "Iteration 280, loss = 1.01662307\n",
            "Iteration 281, loss = 1.01435216\n",
            "Iteration 282, loss = 1.01211432\n",
            "Iteration 283, loss = 1.00989738\n",
            "Iteration 284, loss = 1.00770153\n",
            "Iteration 285, loss = 1.00552671\n",
            "Iteration 286, loss = 1.00337617\n",
            "Iteration 287, loss = 1.00124291\n",
            "Iteration 288, loss = 0.99913576\n",
            "Iteration 289, loss = 0.99704617\n",
            "Iteration 290, loss = 0.99498197\n",
            "Iteration 291, loss = 0.99293407\n",
            "Iteration 292, loss = 0.99091141\n",
            "Iteration 293, loss = 0.98890354\n",
            "Iteration 294, loss = 0.98691877\n",
            "Iteration 295, loss = 0.98496023\n",
            "Iteration 296, loss = 0.98300964\n",
            "Iteration 297, loss = 0.98108247\n",
            "Iteration 298, loss = 0.97917382\n",
            "Iteration 299, loss = 0.97728377\n",
            "Iteration 300, loss = 0.97541283\n",
            "Iteration 301, loss = 0.97355633\n",
            "Iteration 302, loss = 0.97171849\n",
            "Iteration 303, loss = 0.96989621\n",
            "Iteration 304, loss = 0.96809061\n",
            "Iteration 305, loss = 0.96630082\n",
            "Iteration 306, loss = 0.96452621\n",
            "Iteration 307, loss = 0.96276671\n",
            "Iteration 308, loss = 0.96102314\n",
            "Iteration 309, loss = 0.95929415\n",
            "Iteration 310, loss = 0.95758196\n",
            "Iteration 311, loss = 0.95588432\n",
            "Iteration 312, loss = 0.95420101\n",
            "Iteration 313, loss = 0.95253181\n",
            "Iteration 314, loss = 0.95087654\n",
            "Iteration 315, loss = 0.94923501\n",
            "Iteration 316, loss = 0.94760703\n",
            "Iteration 317, loss = 0.94599238\n",
            "Iteration 318, loss = 0.94439091\n",
            "Iteration 319, loss = 0.94280232\n",
            "Iteration 320, loss = 0.94122648\n",
            "Iteration 321, loss = 0.93966318\n",
            "Iteration 322, loss = 0.93811223\n",
            "Iteration 323, loss = 0.93657343\n",
            "Iteration 324, loss = 0.93504660\n",
            "Iteration 325, loss = 0.93353304\n",
            "Iteration 326, loss = 0.93203142\n",
            "Iteration 327, loss = 0.93054093\n",
            "Iteration 328, loss = 0.92906621\n",
            "Iteration 329, loss = 0.92760295\n",
            "Iteration 330, loss = 0.92615093\n",
            "Iteration 331, loss = 0.92471005\n",
            "Iteration 332, loss = 0.92328018\n",
            "Iteration 333, loss = 0.92186112\n",
            "Iteration 334, loss = 0.92045259\n",
            "Iteration 335, loss = 0.91905433\n",
            "Iteration 336, loss = 0.91766706\n",
            "Iteration 337, loss = 0.91629085\n",
            "Iteration 338, loss = 0.91492434\n",
            "Iteration 339, loss = 0.91356733\n",
            "Iteration 340, loss = 0.91221968\n",
            "Iteration 341, loss = 0.91088124\n",
            "Iteration 342, loss = 0.90955192\n",
            "Iteration 343, loss = 0.90823237\n",
            "Iteration 344, loss = 0.90692098\n",
            "Iteration 345, loss = 0.90561929\n",
            "Iteration 346, loss = 0.90432598\n",
            "Iteration 347, loss = 0.90304120\n",
            "Iteration 348, loss = 0.90176529\n",
            "Iteration 349, loss = 0.90049750\n",
            "Iteration 350, loss = 0.89923806\n",
            "Iteration 351, loss = 0.89798707\n",
            "Iteration 352, loss = 0.89674366\n",
            "Iteration 353, loss = 0.89550810\n",
            "Iteration 354, loss = 0.89428075\n",
            "Iteration 355, loss = 0.89306058\n",
            "Iteration 356, loss = 0.89184810\n",
            "Iteration 357, loss = 0.89064317\n",
            "Iteration 358, loss = 0.88944531\n",
            "Iteration 359, loss = 0.88825498\n",
            "Iteration 360, loss = 0.88707161\n",
            "Iteration 361, loss = 0.88589518\n",
            "Iteration 362, loss = 0.88472585\n",
            "Iteration 363, loss = 0.88356342\n",
            "Iteration 364, loss = 0.88240754\n",
            "Iteration 365, loss = 0.88125823\n",
            "Iteration 366, loss = 0.88011589\n",
            "Iteration 367, loss = 0.87897959\n",
            "Iteration 368, loss = 0.87784975\n",
            "Iteration 369, loss = 0.87672625\n",
            "Iteration 370, loss = 0.87561065\n",
            "Iteration 371, loss = 0.87449869\n",
            "Iteration 372, loss = 0.87339456\n",
            "Iteration 373, loss = 0.87229632\n",
            "Iteration 374, loss = 0.87120398\n",
            "Iteration 375, loss = 0.87011748\n",
            "Iteration 376, loss = 0.86903675\n",
            "Iteration 377, loss = 0.86796173\n",
            "Iteration 378, loss = 0.86689234\n",
            "Iteration 379, loss = 0.86582926\n",
            "Iteration 380, loss = 0.86477052\n",
            "Iteration 381, loss = 0.86371776\n",
            "Iteration 382, loss = 0.86267035\n",
            "Iteration 383, loss = 0.86162823\n",
            "Iteration 384, loss = 0.86059145\n",
            "Iteration 385, loss = 0.85955991\n",
            "Iteration 386, loss = 0.85853342\n",
            "Iteration 387, loss = 0.85751196\n",
            "Iteration 388, loss = 0.85649548\n",
            "Iteration 389, loss = 0.85548393\n",
            "Iteration 390, loss = 0.85447726\n",
            "Iteration 391, loss = 0.85347541\n",
            "Iteration 392, loss = 0.85247834\n",
            "Iteration 393, loss = 0.85148600\n",
            "Iteration 394, loss = 0.85049834\n",
            "Iteration 395, loss = 0.84951563\n",
            "Iteration 396, loss = 0.84853697\n",
            "Iteration 397, loss = 0.84756307\n",
            "Iteration 398, loss = 0.84659363\n",
            "Iteration 399, loss = 0.84562861\n",
            "Iteration 400, loss = 0.84466796\n",
            "Iteration 401, loss = 0.84371163\n",
            "Iteration 402, loss = 0.84275958\n",
            "Iteration 403, loss = 0.84181178\n",
            "Iteration 404, loss = 0.84086817\n",
            "Iteration 405, loss = 0.83992871\n",
            "Iteration 406, loss = 0.83899337\n",
            "Iteration 407, loss = 0.83806243\n",
            "Iteration 408, loss = 0.83713602\n",
            "Iteration 409, loss = 0.83621361\n",
            "Iteration 410, loss = 0.83529516\n",
            "Iteration 411, loss = 0.83438064\n",
            "Iteration 412, loss = 0.83347001\n",
            "Iteration 413, loss = 0.83256323\n",
            "Iteration 414, loss = 0.83166027\n",
            "Iteration 415, loss = 0.83076109\n",
            "Iteration 416, loss = 0.82986566\n",
            "Iteration 417, loss = 0.82897394\n",
            "Iteration 418, loss = 0.82808589\n",
            "Iteration 419, loss = 0.82720147\n",
            "Iteration 420, loss = 0.82632066\n",
            "Iteration 421, loss = 0.82544342\n",
            "Iteration 422, loss = 0.82456972\n",
            "Iteration 423, loss = 0.82369952\n",
            "Iteration 424, loss = 0.82283278\n",
            "Iteration 425, loss = 0.82196949\n",
            "Iteration 426, loss = 0.82110960\n",
            "Iteration 427, loss = 0.82025308\n",
            "Iteration 428, loss = 0.81939990\n",
            "Iteration 429, loss = 0.81855003\n",
            "Iteration 430, loss = 0.81770344\n",
            "Iteration 431, loss = 0.81686010\n",
            "Iteration 432, loss = 0.81601997\n",
            "Iteration 433, loss = 0.81518303\n",
            "Iteration 434, loss = 0.81434924\n",
            "Iteration 435, loss = 0.81351858\n",
            "Iteration 436, loss = 0.81269102\n",
            "Iteration 437, loss = 0.81186653\n",
            "Iteration 438, loss = 0.81104508\n",
            "Iteration 439, loss = 0.81022664\n",
            "Iteration 440, loss = 0.80941118\n",
            "Iteration 441, loss = 0.80859868\n",
            "Iteration 442, loss = 0.80778911\n",
            "Iteration 443, loss = 0.80698243\n",
            "Iteration 444, loss = 0.80617864\n",
            "Iteration 445, loss = 0.80537769\n",
            "Iteration 446, loss = 0.80457956\n",
            "Iteration 447, loss = 0.80378573\n",
            "Iteration 448, loss = 0.80299394\n",
            "Iteration 449, loss = 0.80220469\n",
            "Iteration 450, loss = 0.80142039\n",
            "Iteration 451, loss = 0.80063773\n",
            "Iteration 452, loss = 0.79985933\n",
            "Iteration 453, loss = 0.79908289\n",
            "Iteration 454, loss = 0.79830978\n",
            "Iteration 455, loss = 0.79753958\n",
            "Iteration 456, loss = 0.79677222\n",
            "Iteration 457, loss = 0.79600759\n",
            "Iteration 458, loss = 0.79524556\n",
            "Iteration 459, loss = 0.79448601\n",
            "Iteration 460, loss = 0.79372888\n",
            "Iteration 461, loss = 0.79297413\n",
            "Iteration 462, loss = 0.79222174\n",
            "Iteration 463, loss = 0.79147373\n",
            "Iteration 464, loss = 0.79072691\n",
            "Iteration 465, loss = 0.78998322\n",
            "Iteration 466, loss = 0.78924182\n",
            "Iteration 467, loss = 0.78850270\n",
            "Iteration 468, loss = 0.78776619\n",
            "Iteration 469, loss = 0.78703140\n",
            "Iteration 470, loss = 0.78629987\n",
            "Iteration 471, loss = 0.78556988\n",
            "Iteration 472, loss = 0.78484287\n",
            "Iteration 473, loss = 0.78411783\n",
            "Iteration 474, loss = 0.78339467\n",
            "Iteration 475, loss = 0.78267490\n",
            "Iteration 476, loss = 0.78195563\n",
            "Iteration 477, loss = 0.78124088\n",
            "Iteration 478, loss = 0.78052598\n",
            "Iteration 479, loss = 0.77981451\n",
            "Iteration 480, loss = 0.77910458\n",
            "Iteration 481, loss = 0.77839672\n",
            "Iteration 482, loss = 0.77769212\n",
            "Iteration 483, loss = 0.77698785\n",
            "Iteration 484, loss = 0.77628819\n",
            "Iteration 485, loss = 0.77558815\n",
            "Iteration 486, loss = 0.77489108\n",
            "Iteration 487, loss = 0.77419588\n",
            "Iteration 488, loss = 0.77350268\n",
            "Iteration 489, loss = 0.77281218\n",
            "Iteration 490, loss = 0.77212255\n",
            "Iteration 491, loss = 0.77143578\n",
            "Iteration 492, loss = 0.77075095\n",
            "Iteration 493, loss = 0.77006728\n",
            "Iteration 494, loss = 0.76938670\n",
            "Iteration 495, loss = 0.76870668\n",
            "Iteration 496, loss = 0.76802956\n",
            "Iteration 497, loss = 0.76735398\n",
            "Iteration 498, loss = 0.76668010\n",
            "Iteration 499, loss = 0.76600858\n",
            "Iteration 500, loss = 0.76533833\n",
            "Iteration 501, loss = 0.76467008\n",
            "Iteration 502, loss = 0.76400425\n",
            "Iteration 503, loss = 0.76333877\n",
            "Iteration 504, loss = 0.76267620\n",
            "Iteration 505, loss = 0.76201566\n",
            "Iteration 506, loss = 0.76135554\n",
            "Iteration 507, loss = 0.76069819\n",
            "Iteration 508, loss = 0.76004222\n",
            "Iteration 509, loss = 0.75938794\n",
            "Iteration 510, loss = 0.75873584\n",
            "Iteration 511, loss = 0.75808482\n",
            "Iteration 512, loss = 0.75743531\n",
            "Iteration 513, loss = 0.75678830\n",
            "Iteration 514, loss = 0.75614171\n",
            "Iteration 515, loss = 0.75549759\n",
            "Iteration 516, loss = 0.75485553\n",
            "Iteration 517, loss = 0.75421355\n",
            "Iteration 518, loss = 0.75357468\n",
            "Iteration 519, loss = 0.75293611\n",
            "Iteration 520, loss = 0.75229994\n",
            "Iteration 521, loss = 0.75166510\n",
            "Iteration 522, loss = 0.75103155\n",
            "Iteration 523, loss = 0.75039991\n",
            "Iteration 524, loss = 0.74976933\n",
            "Iteration 525, loss = 0.74914103\n",
            "Iteration 526, loss = 0.74851328\n",
            "Iteration 527, loss = 0.74788750\n",
            "Iteration 528, loss = 0.74726321\n",
            "Iteration 529, loss = 0.74663992\n",
            "Iteration 530, loss = 0.74601872\n",
            "Iteration 531, loss = 0.74539810\n",
            "Iteration 532, loss = 0.74477976\n",
            "Iteration 533, loss = 0.74416206\n",
            "Iteration 534, loss = 0.74354623\n",
            "Iteration 535, loss = 0.74293161\n",
            "Iteration 536, loss = 0.74231817\n",
            "Iteration 537, loss = 0.74170659\n",
            "Iteration 538, loss = 0.74109559\n",
            "Iteration 539, loss = 0.74048697\n",
            "Iteration 540, loss = 0.73987865\n",
            "Iteration 541, loss = 0.73927201\n",
            "Iteration 542, loss = 0.73866638\n",
            "Iteration 543, loss = 0.73806271\n",
            "Iteration 544, loss = 0.73745941\n",
            "Iteration 545, loss = 0.73685841\n",
            "Iteration 546, loss = 0.73625806\n",
            "Iteration 547, loss = 0.73565997\n",
            "Iteration 548, loss = 0.73506345\n",
            "Iteration 549, loss = 0.73446767\n",
            "Iteration 550, loss = 0.73387404\n",
            "Iteration 551, loss = 0.73328070\n",
            "Iteration 552, loss = 0.73268965\n",
            "Iteration 553, loss = 0.73209917\n",
            "Iteration 554, loss = 0.73150959\n",
            "Iteration 555, loss = 0.73092118\n",
            "Iteration 556, loss = 0.73033409\n",
            "Iteration 557, loss = 0.72974772\n",
            "Iteration 558, loss = 0.72916288\n",
            "Iteration 559, loss = 0.72857859\n",
            "Iteration 560, loss = 0.72799540\n",
            "Iteration 561, loss = 0.72741407\n",
            "Iteration 562, loss = 0.72683317\n",
            "Iteration 563, loss = 0.72625371\n",
            "Iteration 564, loss = 0.72567541\n",
            "Iteration 565, loss = 0.72509776\n",
            "Iteration 566, loss = 0.72452141\n",
            "Iteration 567, loss = 0.72394593\n",
            "Iteration 568, loss = 0.72337131\n",
            "Iteration 569, loss = 0.72279825\n",
            "Iteration 570, loss = 0.72222556\n",
            "Iteration 571, loss = 0.72165399\n",
            "Iteration 572, loss = 0.72108350\n",
            "Iteration 573, loss = 0.72051364\n",
            "Iteration 574, loss = 0.71994545\n",
            "Iteration 575, loss = 0.71937718\n",
            "Iteration 576, loss = 0.71881054\n",
            "Iteration 577, loss = 0.71824459\n",
            "Iteration 578, loss = 0.71767953\n",
            "Iteration 579, loss = 0.71711546\n",
            "Iteration 580, loss = 0.71655255\n",
            "Iteration 581, loss = 0.71599024\n",
            "Iteration 582, loss = 0.71542866\n",
            "Iteration 583, loss = 0.71486837\n",
            "Iteration 584, loss = 0.71430857\n",
            "Iteration 585, loss = 0.71374959\n",
            "Iteration 586, loss = 0.71319174\n",
            "Iteration 587, loss = 0.71263456\n",
            "Iteration 588, loss = 0.71207805\n",
            "Iteration 589, loss = 0.71152225\n",
            "Iteration 590, loss = 0.71096810\n",
            "Iteration 591, loss = 0.71041450\n",
            "Iteration 592, loss = 0.70986192\n",
            "Iteration 593, loss = 0.70931020\n",
            "Iteration 594, loss = 0.70875951\n",
            "Iteration 595, loss = 0.70820909\n",
            "Iteration 596, loss = 0.70765968\n",
            "Iteration 597, loss = 0.70711115\n",
            "Iteration 598, loss = 0.70656324\n",
            "Iteration 599, loss = 0.70601651\n",
            "Iteration 600, loss = 0.70546968\n",
            "Iteration 601, loss = 0.70492432\n",
            "Iteration 602, loss = 0.70437992\n",
            "Iteration 603, loss = 0.70383491\n",
            "Iteration 604, loss = 0.70329252\n",
            "Iteration 605, loss = 0.70274915\n",
            "Iteration 606, loss = 0.70220697\n",
            "Iteration 607, loss = 0.70166643\n",
            "Iteration 608, loss = 0.70112553\n",
            "Iteration 609, loss = 0.70058525\n",
            "Iteration 610, loss = 0.70004646\n",
            "Iteration 611, loss = 0.69950740\n",
            "Iteration 612, loss = 0.69896975\n",
            "Iteration 613, loss = 0.69843232\n",
            "Iteration 614, loss = 0.69789567\n",
            "Iteration 615, loss = 0.69736008\n",
            "Iteration 616, loss = 0.69682486\n",
            "Iteration 617, loss = 0.69629039\n",
            "Iteration 618, loss = 0.69575608\n",
            "Iteration 619, loss = 0.69522282\n",
            "Iteration 620, loss = 0.69469037\n",
            "Iteration 621, loss = 0.69415820\n",
            "Iteration 622, loss = 0.69362603\n",
            "Iteration 623, loss = 0.69309597\n",
            "Iteration 624, loss = 0.69256504\n",
            "Iteration 625, loss = 0.69203557\n",
            "Iteration 626, loss = 0.69150575\n",
            "Iteration 627, loss = 0.69097759\n",
            "Iteration 628, loss = 0.69044885\n",
            "Iteration 629, loss = 0.68992237\n",
            "Iteration 630, loss = 0.68939438\n",
            "Iteration 631, loss = 0.68886815\n",
            "Iteration 632, loss = 0.68834172\n",
            "Iteration 633, loss = 0.68781648\n",
            "Iteration 634, loss = 0.68729093\n",
            "Iteration 635, loss = 0.68676701\n",
            "Iteration 636, loss = 0.68624319\n",
            "Iteration 637, loss = 0.68571909\n",
            "Iteration 638, loss = 0.68519621\n",
            "Iteration 639, loss = 0.68467313\n",
            "Iteration 640, loss = 0.68415165\n",
            "Iteration 641, loss = 0.68362981\n",
            "Iteration 642, loss = 0.68310825\n",
            "Iteration 643, loss = 0.68258811\n",
            "Iteration 644, loss = 0.68206712\n",
            "Iteration 645, loss = 0.68154769\n",
            "Iteration 646, loss = 0.68102795\n",
            "Iteration 647, loss = 0.68050847\n",
            "Iteration 648, loss = 0.67999044\n",
            "Iteration 649, loss = 0.67947177\n",
            "Iteration 650, loss = 0.67895359\n",
            "Iteration 651, loss = 0.67843636\n",
            "Iteration 652, loss = 0.67791903\n",
            "Iteration 653, loss = 0.67740382\n",
            "Iteration 654, loss = 0.67689007\n",
            "Iteration 655, loss = 0.67637754\n",
            "Iteration 656, loss = 0.67586544\n",
            "Iteration 657, loss = 0.67535396\n",
            "Iteration 658, loss = 0.67484295\n",
            "Iteration 659, loss = 0.67433239\n",
            "Iteration 660, loss = 0.67382228\n",
            "Iteration 661, loss = 0.67331266\n",
            "Iteration 662, loss = 0.67280398\n",
            "Iteration 663, loss = 0.67229544\n",
            "Iteration 664, loss = 0.67178741\n",
            "Iteration 665, loss = 0.67127913\n",
            "Iteration 666, loss = 0.67077176\n",
            "Iteration 667, loss = 0.67026469\n",
            "Iteration 668, loss = 0.66975798\n",
            "Iteration 669, loss = 0.66925204\n",
            "Iteration 670, loss = 0.66874609\n",
            "Iteration 671, loss = 0.66824038\n",
            "Iteration 672, loss = 0.66773495\n",
            "Iteration 673, loss = 0.66722970\n",
            "Iteration 674, loss = 0.66672566\n",
            "Iteration 675, loss = 0.66622057\n",
            "Iteration 676, loss = 0.66571650\n",
            "Iteration 677, loss = 0.66521320\n",
            "Iteration 678, loss = 0.66470921\n",
            "Iteration 679, loss = 0.66420566\n",
            "Iteration 680, loss = 0.66370283\n",
            "Iteration 681, loss = 0.66319977\n",
            "Iteration 682, loss = 0.66269704\n",
            "Iteration 683, loss = 0.66219446\n",
            "Iteration 684, loss = 0.66169270\n",
            "Iteration 685, loss = 0.66119065\n",
            "Iteration 686, loss = 0.66068869\n",
            "Iteration 687, loss = 0.66018692\n",
            "Iteration 688, loss = 0.65968585\n",
            "Iteration 689, loss = 0.65918450\n",
            "Iteration 690, loss = 0.65868346\n",
            "Iteration 691, loss = 0.65818277\n",
            "Iteration 692, loss = 0.65768196\n",
            "Iteration 693, loss = 0.65718169\n",
            "Iteration 694, loss = 0.65668116\n",
            "Iteration 695, loss = 0.65618088\n",
            "Iteration 696, loss = 0.65568129\n",
            "Iteration 697, loss = 0.65518120\n",
            "Iteration 698, loss = 0.65468118\n",
            "Iteration 699, loss = 0.65418193\n",
            "Iteration 700, loss = 0.65368214\n",
            "Iteration 701, loss = 0.65318314\n",
            "Iteration 702, loss = 0.65268343\n",
            "Iteration 703, loss = 0.65218471\n",
            "Iteration 704, loss = 0.65168539\n",
            "Iteration 705, loss = 0.65118643\n",
            "Iteration 706, loss = 0.65068787\n",
            "Iteration 707, loss = 0.65018883\n",
            "Iteration 708, loss = 0.64969042\n",
            "Iteration 709, loss = 0.64919146\n",
            "Iteration 710, loss = 0.64869333\n",
            "Iteration 711, loss = 0.64819450\n",
            "Iteration 712, loss = 0.64769642\n",
            "Iteration 713, loss = 0.64719791\n",
            "Iteration 714, loss = 0.64669967\n",
            "Iteration 715, loss = 0.64620152\n",
            "Iteration 716, loss = 0.64570314\n",
            "Iteration 717, loss = 0.64520534\n",
            "Iteration 718, loss = 0.64470677\n",
            "Iteration 719, loss = 0.64420922\n",
            "Iteration 720, loss = 0.64371065\n",
            "Iteration 721, loss = 0.64321338\n",
            "Iteration 722, loss = 0.64271509\n",
            "Iteration 723, loss = 0.64221660\n",
            "Iteration 724, loss = 0.64171899\n",
            "Iteration 725, loss = 0.64122059\n",
            "Iteration 726, loss = 0.64072306\n",
            "Iteration 727, loss = 0.64022456\n",
            "Iteration 728, loss = 0.63972727\n",
            "Iteration 729, loss = 0.63922874\n",
            "Iteration 730, loss = 0.63873085\n",
            "Iteration 731, loss = 0.63823250\n",
            "Iteration 732, loss = 0.63773452\n",
            "Iteration 733, loss = 0.63723625\n",
            "Iteration 734, loss = 0.63673794\n",
            "Iteration 735, loss = 0.63624002\n",
            "Iteration 736, loss = 0.63574146\n",
            "Iteration 737, loss = 0.63524330\n",
            "Iteration 738, loss = 0.63474471\n",
            "Iteration 739, loss = 0.63424619\n",
            "Iteration 740, loss = 0.63374796\n",
            "Iteration 741, loss = 0.63324912\n",
            "Iteration 742, loss = 0.63275042\n",
            "Iteration 743, loss = 0.63225180\n",
            "Iteration 744, loss = 0.63175270\n",
            "Iteration 745, loss = 0.63125416\n",
            "Iteration 746, loss = 0.63075481\n",
            "Iteration 747, loss = 0.63025555\n",
            "Iteration 748, loss = 0.62975684\n",
            "Iteration 749, loss = 0.62925740\n",
            "Iteration 750, loss = 0.62875770\n",
            "Iteration 751, loss = 0.62825838\n",
            "Iteration 752, loss = 0.62775854\n",
            "Iteration 753, loss = 0.62725856\n",
            "Iteration 754, loss = 0.62675918\n",
            "Iteration 755, loss = 0.62625876\n",
            "Iteration 756, loss = 0.62575847\n",
            "Iteration 757, loss = 0.62525847\n",
            "Iteration 758, loss = 0.62475798\n",
            "Iteration 759, loss = 0.62425730\n",
            "Iteration 760, loss = 0.62375636\n",
            "Iteration 761, loss = 0.62325591\n",
            "Iteration 762, loss = 0.62275458\n",
            "Iteration 763, loss = 0.62225331\n",
            "Iteration 764, loss = 0.62175182\n",
            "Iteration 765, loss = 0.62125069\n",
            "Iteration 766, loss = 0.62074895\n",
            "Iteration 767, loss = 0.62024694\n",
            "Iteration 768, loss = 0.61974469\n",
            "Iteration 769, loss = 0.61924259\n",
            "Iteration 770, loss = 0.61874015\n",
            "Iteration 771, loss = 0.61823751\n",
            "Iteration 772, loss = 0.61773457\n",
            "Iteration 773, loss = 0.61723148\n",
            "Iteration 774, loss = 0.61672859\n",
            "Iteration 775, loss = 0.61622517\n",
            "Iteration 776, loss = 0.61572149\n",
            "Iteration 777, loss = 0.61521754\n",
            "Iteration 778, loss = 0.61471336\n",
            "Iteration 779, loss = 0.61420937\n",
            "Iteration 780, loss = 0.61370478\n",
            "Iteration 781, loss = 0.61320007\n",
            "Iteration 782, loss = 0.61269509\n",
            "Iteration 783, loss = 0.61218985\n",
            "Iteration 784, loss = 0.61168436\n",
            "Iteration 785, loss = 0.61117905\n",
            "Iteration 786, loss = 0.61067304\n",
            "Iteration 787, loss = 0.61016694\n",
            "Iteration 788, loss = 0.60966056\n",
            "Iteration 789, loss = 0.60915390\n",
            "Iteration 790, loss = 0.60864697\n",
            "Iteration 791, loss = 0.60813978\n",
            "Iteration 792, loss = 0.60763233\n",
            "Iteration 793, loss = 0.60712492\n",
            "Iteration 794, loss = 0.60661692\n",
            "Iteration 795, loss = 0.60610877\n",
            "Iteration 796, loss = 0.60560033\n",
            "Iteration 797, loss = 0.60509158\n",
            "Iteration 798, loss = 0.60458255\n",
            "Iteration 799, loss = 0.60407323\n",
            "Iteration 800, loss = 0.60356362\n",
            "Iteration 801, loss = 0.60305373\n",
            "Iteration 802, loss = 0.60254355\n",
            "Iteration 803, loss = 0.60203309\n",
            "Iteration 804, loss = 0.60152233\n",
            "Iteration 805, loss = 0.60101132\n",
            "Iteration 806, loss = 0.60049996\n",
            "Iteration 807, loss = 0.59998832\n",
            "Iteration 808, loss = 0.59947664\n",
            "Iteration 809, loss = 0.59896416\n",
            "Iteration 810, loss = 0.59845163\n",
            "Iteration 811, loss = 0.59793876\n",
            "Iteration 812, loss = 0.59742556\n",
            "Iteration 813, loss = 0.59691203\n",
            "Iteration 814, loss = 0.59639815\n",
            "Iteration 815, loss = 0.59588394\n",
            "Iteration 816, loss = 0.59536939\n",
            "Iteration 817, loss = 0.59485452\n",
            "Iteration 818, loss = 0.59433931\n",
            "Iteration 819, loss = 0.59382377\n",
            "Iteration 820, loss = 0.59330789\n",
            "Iteration 821, loss = 0.59279166\n",
            "Iteration 822, loss = 0.59227509\n",
            "Iteration 823, loss = 0.59175817\n",
            "Iteration 824, loss = 0.59124089\n",
            "Iteration 825, loss = 0.59072328\n",
            "Iteration 826, loss = 0.59020527\n",
            "Iteration 827, loss = 0.58968691\n",
            "Iteration 828, loss = 0.58916819\n",
            "Iteration 829, loss = 0.58864910\n",
            "Iteration 830, loss = 0.58812963\n",
            "Iteration 831, loss = 0.58760980\n",
            "Iteration 832, loss = 0.58708959\n",
            "Iteration 833, loss = 0.58656899\n",
            "Iteration 834, loss = 0.58604802\n",
            "Iteration 835, loss = 0.58552666\n",
            "Iteration 836, loss = 0.58500495\n",
            "Iteration 837, loss = 0.58448301\n",
            "Iteration 838, loss = 0.58396068\n",
            "Iteration 839, loss = 0.58343796\n",
            "Iteration 840, loss = 0.58291485\n",
            "Iteration 841, loss = 0.58239135\n",
            "Iteration 842, loss = 0.58186745\n",
            "Iteration 843, loss = 0.58134321\n",
            "Iteration 844, loss = 0.58081865\n",
            "Iteration 845, loss = 0.58029369\n",
            "Iteration 846, loss = 0.57976833\n",
            "Iteration 847, loss = 0.57924255\n",
            "Iteration 848, loss = 0.57871637\n",
            "Iteration 849, loss = 0.57818976\n",
            "Iteration 850, loss = 0.57766274\n",
            "Iteration 851, loss = 0.57713530\n",
            "Iteration 852, loss = 0.57660744\n",
            "Iteration 853, loss = 0.57607915\n",
            "Iteration 854, loss = 0.57555043\n",
            "Iteration 855, loss = 0.57502129\n",
            "Iteration 856, loss = 0.57449171\n",
            "Iteration 857, loss = 0.57396169\n",
            "Iteration 858, loss = 0.57343124\n",
            "Iteration 859, loss = 0.57290035\n",
            "Iteration 860, loss = 0.57236902\n",
            "Iteration 861, loss = 0.57183724\n",
            "Iteration 862, loss = 0.57130502\n",
            "Iteration 863, loss = 0.57077235\n",
            "Iteration 864, loss = 0.57023924\n",
            "Iteration 865, loss = 0.56970567\n",
            "Iteration 866, loss = 0.56917165\n",
            "Iteration 867, loss = 0.56863717\n",
            "Iteration 868, loss = 0.56810224\n",
            "Iteration 869, loss = 0.56756684\n",
            "Iteration 870, loss = 0.56703099\n",
            "Iteration 871, loss = 0.56649468\n",
            "Iteration 872, loss = 0.56595790\n",
            "Iteration 873, loss = 0.56542065\n",
            "Iteration 874, loss = 0.56488294\n",
            "Iteration 875, loss = 0.56434477\n",
            "Iteration 876, loss = 0.56380612\n",
            "Iteration 877, loss = 0.56326700\n",
            "Iteration 878, loss = 0.56272741\n",
            "Iteration 879, loss = 0.56218734\n",
            "Iteration 880, loss = 0.56164680\n",
            "Iteration 881, loss = 0.56110578\n",
            "Iteration 882, loss = 0.56056429\n",
            "Iteration 883, loss = 0.56002231\n",
            "Iteration 884, loss = 0.55947986\n",
            "Iteration 885, loss = 0.55893692\n",
            "Iteration 886, loss = 0.55839350\n",
            "Iteration 887, loss = 0.55784960\n",
            "Iteration 888, loss = 0.55730521\n",
            "Iteration 889, loss = 0.55676034\n",
            "Iteration 890, loss = 0.55621498\n",
            "Iteration 891, loss = 0.55566914\n",
            "Iteration 892, loss = 0.55512522\n",
            "Iteration 893, loss = 0.55457939\n",
            "Iteration 894, loss = 0.55403228\n",
            "Iteration 895, loss = 0.55348541\n",
            "Iteration 896, loss = 0.55293838\n",
            "Iteration 897, loss = 0.55239180\n",
            "Iteration 898, loss = 0.55184538\n",
            "Iteration 899, loss = 0.55129733\n",
            "Iteration 900, loss = 0.55074829\n",
            "Iteration 901, loss = 0.55019896\n",
            "Iteration 902, loss = 0.54964974\n",
            "Iteration 903, loss = 0.54910132\n",
            "Iteration 904, loss = 0.54855116\n",
            "Iteration 905, loss = 0.54800066\n",
            "Iteration 906, loss = 0.54745031\n",
            "Iteration 907, loss = 0.54689955\n",
            "Iteration 908, loss = 0.54634805\n",
            "Iteration 909, loss = 0.54579690\n",
            "Iteration 910, loss = 0.54524438\n",
            "Iteration 911, loss = 0.54469264\n",
            "Iteration 912, loss = 0.54413929\n",
            "Iteration 913, loss = 0.54358673\n",
            "Iteration 914, loss = 0.54303278\n",
            "Iteration 915, loss = 0.54247908\n",
            "Iteration 916, loss = 0.54192487\n",
            "Iteration 917, loss = 0.54136977\n",
            "Iteration 918, loss = 0.54081556\n",
            "Iteration 919, loss = 0.54025923\n",
            "Iteration 920, loss = 0.53970409\n",
            "Iteration 921, loss = 0.53914752\n",
            "Iteration 922, loss = 0.53859067\n",
            "Iteration 923, loss = 0.53803433\n",
            "Iteration 924, loss = 0.53747655\n",
            "Iteration 925, loss = 0.53691857\n",
            "Iteration 926, loss = 0.53636111\n",
            "Iteration 927, loss = 0.53580218\n",
            "Iteration 928, loss = 0.53524310\n",
            "Iteration 929, loss = 0.53468413\n",
            "Iteration 930, loss = 0.53412451\n",
            "Iteration 931, loss = 0.53356434\n",
            "Iteration 932, loss = 0.53300365\n",
            "Iteration 933, loss = 0.53244347\n",
            "Iteration 934, loss = 0.53188281\n",
            "Iteration 935, loss = 0.53132051\n",
            "Iteration 936, loss = 0.53075908\n",
            "Iteration 937, loss = 0.53019689\n",
            "Iteration 938, loss = 0.52963453\n",
            "Iteration 939, loss = 0.52907158\n",
            "Iteration 940, loss = 0.52850871\n",
            "Iteration 941, loss = 0.52794475\n",
            "Iteration 942, loss = 0.52738125\n",
            "Iteration 943, loss = 0.52681656\n",
            "Iteration 944, loss = 0.52625162\n",
            "Iteration 945, loss = 0.52568709\n",
            "Iteration 946, loss = 0.52512178\n",
            "Iteration 947, loss = 0.52455641\n",
            "Iteration 948, loss = 0.52399023\n",
            "Iteration 949, loss = 0.52342353\n",
            "Iteration 950, loss = 0.52285756\n",
            "Iteration 951, loss = 0.52229089\n",
            "Iteration 952, loss = 0.52172314\n",
            "Iteration 953, loss = 0.52115629\n",
            "Iteration 954, loss = 0.52058789\n",
            "Iteration 955, loss = 0.52001949\n",
            "Iteration 956, loss = 0.51945060\n",
            "Iteration 957, loss = 0.51888188\n",
            "Iteration 958, loss = 0.51831210\n",
            "Iteration 959, loss = 0.51774280\n",
            "Iteration 960, loss = 0.51717358\n",
            "Iteration 961, loss = 0.51660316\n",
            "Iteration 962, loss = 0.51603197\n",
            "Iteration 963, loss = 0.51546086\n",
            "Iteration 964, loss = 0.51488920\n",
            "Iteration 965, loss = 0.51431824\n",
            "Iteration 966, loss = 0.51374562\n",
            "Iteration 967, loss = 0.51317407\n",
            "Iteration 968, loss = 0.51260108\n",
            "Iteration 969, loss = 0.51202790\n",
            "Iteration 970, loss = 0.51145474\n",
            "Iteration 971, loss = 0.51088118\n",
            "Iteration 972, loss = 0.51030730\n",
            "Iteration 973, loss = 0.50973309\n",
            "Iteration 974, loss = 0.50915874\n",
            "Iteration 975, loss = 0.50858368\n",
            "Iteration 976, loss = 0.50800902\n",
            "Iteration 977, loss = 0.50743315\n",
            "Iteration 978, loss = 0.50685819\n",
            "Iteration 979, loss = 0.50628160\n",
            "Iteration 980, loss = 0.50570575\n",
            "Iteration 981, loss = 0.50512883\n",
            "Iteration 982, loss = 0.50455196\n",
            "Iteration 983, loss = 0.50397469\n",
            "Iteration 984, loss = 0.50339722\n",
            "Iteration 985, loss = 0.50281954\n",
            "Iteration 986, loss = 0.50224111\n",
            "Iteration 987, loss = 0.50166331\n",
            "Iteration 988, loss = 0.50108421\n",
            "Iteration 989, loss = 0.50050562\n",
            "Iteration 990, loss = 0.49992582\n",
            "Iteration 991, loss = 0.49934679\n",
            "Iteration 992, loss = 0.49876653\n",
            "Iteration 993, loss = 0.49818649\n",
            "Iteration 994, loss = 0.49760625\n",
            "Iteration 995, loss = 0.49702515\n",
            "Iteration 996, loss = 0.49644471\n",
            "Iteration 997, loss = 0.49586277\n",
            "Iteration 998, loss = 0.49528172\n",
            "Iteration 999, loss = 0.49469949\n",
            "Iteration 1000, loss = 0.49411735\n",
            "Iteration 1001, loss = 0.49353522\n",
            "Iteration 1002, loss = 0.49295213\n",
            "Iteration 1003, loss = 0.49236958\n",
            "Iteration 1004, loss = 0.49178577\n",
            "Iteration 1005, loss = 0.49120269\n",
            "Iteration 1006, loss = 0.49061856\n",
            "Iteration 1007, loss = 0.49003445\n",
            "Iteration 1008, loss = 0.48945141\n",
            "Iteration 1009, loss = 0.48886710\n",
            "Iteration 1010, loss = 0.48828377\n",
            "Iteration 1011, loss = 0.48769885\n",
            "Iteration 1012, loss = 0.48711627\n",
            "Iteration 1013, loss = 0.48653071\n",
            "Iteration 1014, loss = 0.48594595\n",
            "Iteration 1015, loss = 0.48536121\n",
            "Iteration 1016, loss = 0.48477651\n",
            "Iteration 1017, loss = 0.48419114\n",
            "Iteration 1018, loss = 0.48360738\n",
            "Iteration 1019, loss = 0.48302114\n",
            "Iteration 1020, loss = 0.48243539\n",
            "Iteration 1021, loss = 0.48185039\n",
            "Iteration 1022, loss = 0.48126461\n",
            "Iteration 1023, loss = 0.48067863\n",
            "Iteration 1024, loss = 0.48009253\n",
            "Iteration 1025, loss = 0.47950599\n",
            "Iteration 1026, loss = 0.47892106\n",
            "Iteration 1027, loss = 0.47833362\n",
            "Iteration 1028, loss = 0.47774843\n",
            "Iteration 1029, loss = 0.47716071\n",
            "Iteration 1030, loss = 0.47657459\n",
            "Iteration 1031, loss = 0.47598827\n",
            "Iteration 1032, loss = 0.47540013\n",
            "Iteration 1033, loss = 0.47481568\n",
            "Iteration 1034, loss = 0.47422696\n",
            "Iteration 1035, loss = 0.47363994\n",
            "Iteration 1036, loss = 0.47305306\n",
            "Iteration 1037, loss = 0.47246687\n",
            "Iteration 1038, loss = 0.47188101\n",
            "Iteration 1039, loss = 0.47129314\n",
            "Iteration 1040, loss = 0.47070846\n",
            "Iteration 1041, loss = 0.47012245\n",
            "Iteration 1042, loss = 0.46953710\n",
            "Iteration 1043, loss = 0.46895104\n",
            "Iteration 1044, loss = 0.46836514\n",
            "Iteration 1045, loss = 0.46777899\n",
            "Iteration 1046, loss = 0.46719359\n",
            "Iteration 1047, loss = 0.46660729\n",
            "Iteration 1048, loss = 0.46602182\n",
            "Iteration 1049, loss = 0.46543629\n",
            "Iteration 1050, loss = 0.46484974\n",
            "Iteration 1051, loss = 0.46426344\n",
            "Iteration 1052, loss = 0.46367833\n",
            "Iteration 1053, loss = 0.46309151\n",
            "Iteration 1054, loss = 0.46250568\n",
            "Iteration 1055, loss = 0.46191960\n",
            "Iteration 1056, loss = 0.46133266\n",
            "Iteration 1057, loss = 0.46074671\n",
            "Iteration 1058, loss = 0.46016066\n",
            "Iteration 1059, loss = 0.45957392\n",
            "Iteration 1060, loss = 0.45898790\n",
            "Iteration 1061, loss = 0.45840100\n",
            "Iteration 1062, loss = 0.45781598\n",
            "Iteration 1063, loss = 0.45722868\n",
            "Iteration 1064, loss = 0.45664251\n",
            "Iteration 1065, loss = 0.45605533\n",
            "Iteration 1066, loss = 0.45547010\n",
            "Iteration 1067, loss = 0.45488291\n",
            "Iteration 1068, loss = 0.45429587\n",
            "Iteration 1069, loss = 0.45370987\n",
            "Iteration 1070, loss = 0.45312324\n",
            "Iteration 1071, loss = 0.45253691\n",
            "Iteration 1072, loss = 0.45195067\n",
            "Iteration 1073, loss = 0.45136426\n",
            "Iteration 1074, loss = 0.45077717\n",
            "Iteration 1075, loss = 0.45019100\n",
            "Iteration 1076, loss = 0.44960475\n",
            "Iteration 1077, loss = 0.44901862\n",
            "Iteration 1078, loss = 0.44843157\n",
            "Iteration 1079, loss = 0.44784520\n",
            "Iteration 1080, loss = 0.44725856\n",
            "Iteration 1081, loss = 0.44667286\n",
            "Iteration 1082, loss = 0.44608660\n",
            "Iteration 1083, loss = 0.44550069\n",
            "Iteration 1084, loss = 0.44491373\n",
            "Iteration 1085, loss = 0.44432758\n",
            "Iteration 1086, loss = 0.44374203\n",
            "Iteration 1087, loss = 0.44315578\n",
            "Iteration 1088, loss = 0.44257022\n",
            "Iteration 1089, loss = 0.44198456\n",
            "Iteration 1090, loss = 0.44139860\n",
            "Iteration 1091, loss = 0.44081289\n",
            "Iteration 1092, loss = 0.44022721\n",
            "Iteration 1093, loss = 0.43964218\n",
            "Iteration 1094, loss = 0.43905652\n",
            "Iteration 1095, loss = 0.43847095\n",
            "Iteration 1096, loss = 0.43788535\n",
            "Iteration 1097, loss = 0.43730114\n",
            "Iteration 1098, loss = 0.43671532\n",
            "Iteration 1099, loss = 0.43613078\n",
            "Iteration 1100, loss = 0.43554681\n",
            "Iteration 1101, loss = 0.43496145\n",
            "Iteration 1102, loss = 0.43437681\n",
            "Iteration 1103, loss = 0.43379263\n",
            "Iteration 1104, loss = 0.43320840\n",
            "Iteration 1105, loss = 0.43262432\n",
            "Iteration 1106, loss = 0.43204023\n",
            "Iteration 1107, loss = 0.43145659\n",
            "Iteration 1108, loss = 0.43087270\n",
            "Iteration 1109, loss = 0.43028995\n",
            "Iteration 1110, loss = 0.42970584\n",
            "Iteration 1111, loss = 0.42912255\n",
            "Iteration 1112, loss = 0.42853955\n",
            "Iteration 1113, loss = 0.42795724\n",
            "Iteration 1114, loss = 0.42737428\n",
            "Iteration 1115, loss = 0.42679178\n",
            "Iteration 1116, loss = 0.42620939\n",
            "Iteration 1117, loss = 0.42562715\n",
            "Iteration 1118, loss = 0.42504510\n",
            "Iteration 1119, loss = 0.42446371\n",
            "Iteration 1120, loss = 0.42388324\n",
            "Iteration 1121, loss = 0.42330074\n",
            "Iteration 1122, loss = 0.42271963\n",
            "Iteration 1123, loss = 0.42213906\n",
            "Iteration 1124, loss = 0.42155838\n",
            "Iteration 1125, loss = 0.42097799\n",
            "Iteration 1126, loss = 0.42039776\n",
            "Iteration 1127, loss = 0.41981771\n",
            "Iteration 1128, loss = 0.41923787\n",
            "Iteration 1129, loss = 0.41865828\n",
            "Iteration 1130, loss = 0.41807956\n",
            "Iteration 1131, loss = 0.41750026\n",
            "Iteration 1132, loss = 0.41692155\n",
            "Iteration 1133, loss = 0.41634304\n",
            "Iteration 1134, loss = 0.41576476\n",
            "Iteration 1135, loss = 0.41518674\n",
            "Iteration 1136, loss = 0.41460940\n",
            "Iteration 1137, loss = 0.41403188\n",
            "Iteration 1138, loss = 0.41345477\n",
            "Iteration 1139, loss = 0.41287787\n",
            "Iteration 1140, loss = 0.41230121\n",
            "Iteration 1141, loss = 0.41172482\n",
            "Iteration 1142, loss = 0.41114871\n",
            "Iteration 1143, loss = 0.41057349\n",
            "Iteration 1144, loss = 0.40999767\n",
            "Iteration 1145, loss = 0.40942252\n",
            "Iteration 1146, loss = 0.40884760\n",
            "Iteration 1147, loss = 0.40827295\n",
            "Iteration 1148, loss = 0.40769857\n",
            "Iteration 1149, loss = 0.40712448\n",
            "Iteration 1150, loss = 0.40655098\n",
            "Iteration 1151, loss = 0.40597753\n",
            "Iteration 1152, loss = 0.40540445\n",
            "Iteration 1153, loss = 0.40483162\n",
            "Iteration 1154, loss = 0.40425906\n",
            "Iteration 1155, loss = 0.40368680\n",
            "Iteration 1156, loss = 0.40311485\n",
            "Iteration 1157, loss = 0.40254323\n",
            "Iteration 1158, loss = 0.40197215\n",
            "Iteration 1159, loss = 0.40140121\n",
            "Iteration 1160, loss = 0.40083065\n",
            "Iteration 1161, loss = 0.40026037\n",
            "Iteration 1162, loss = 0.39969040\n",
            "Iteration 1163, loss = 0.39912073\n",
            "Iteration 1164, loss = 0.39855140\n",
            "Iteration 1165, loss = 0.39798242\n",
            "Iteration 1166, loss = 0.39741380\n",
            "Iteration 1167, loss = 0.39684554\n",
            "Iteration 1168, loss = 0.39627767\n",
            "Iteration 1169, loss = 0.39571019\n",
            "Iteration 1170, loss = 0.39514504\n",
            "Iteration 1171, loss = 0.39457836\n",
            "Iteration 1172, loss = 0.39401235\n",
            "Iteration 1173, loss = 0.39344719\n",
            "Iteration 1174, loss = 0.39288307\n",
            "Iteration 1175, loss = 0.39231818\n",
            "Iteration 1176, loss = 0.39175475\n",
            "Iteration 1177, loss = 0.39119068\n",
            "Iteration 1178, loss = 0.39062814\n",
            "Iteration 1179, loss = 0.39006503\n",
            "Iteration 1180, loss = 0.38950342\n",
            "Iteration 1181, loss = 0.38894119\n",
            "Iteration 1182, loss = 0.38838106\n",
            "Iteration 1183, loss = 0.38782016\n",
            "Iteration 1184, loss = 0.38725965\n",
            "Iteration 1185, loss = 0.38670055\n",
            "Iteration 1186, loss = 0.38614096\n",
            "Iteration 1187, loss = 0.38558307\n",
            "Iteration 1188, loss = 0.38502443\n",
            "Iteration 1189, loss = 0.38446756\n",
            "Iteration 1190, loss = 0.38390996\n",
            "Iteration 1191, loss = 0.38335353\n",
            "Iteration 1192, loss = 0.38279716\n",
            "Iteration 1193, loss = 0.38224198\n",
            "Iteration 1194, loss = 0.38168683\n",
            "Iteration 1195, loss = 0.38113207\n",
            "Iteration 1196, loss = 0.38057892\n",
            "Iteration 1197, loss = 0.38002493\n",
            "Iteration 1198, loss = 0.37947189\n",
            "Iteration 1199, loss = 0.37892028\n",
            "Iteration 1200, loss = 0.37836842\n",
            "Iteration 1201, loss = 0.37781697\n",
            "Iteration 1202, loss = 0.37726601\n",
            "Iteration 1203, loss = 0.37671644\n",
            "Iteration 1204, loss = 0.37616644\n",
            "Iteration 1205, loss = 0.37561735\n",
            "Iteration 1206, loss = 0.37506877\n",
            "Iteration 1207, loss = 0.37452149\n",
            "Iteration 1208, loss = 0.37397384\n",
            "Iteration 1209, loss = 0.37342710\n",
            "Iteration 1210, loss = 0.37288086\n",
            "Iteration 1211, loss = 0.37233518\n",
            "Iteration 1212, loss = 0.37179010\n",
            "Iteration 1213, loss = 0.37124600\n",
            "Iteration 1214, loss = 0.37070217\n",
            "Iteration 1215, loss = 0.37015888\n",
            "Iteration 1216, loss = 0.36961616\n",
            "Iteration 1217, loss = 0.36907404\n",
            "Iteration 1218, loss = 0.36853255\n",
            "Iteration 1219, loss = 0.36799167\n",
            "Iteration 1220, loss = 0.36745142\n",
            "Iteration 1221, loss = 0.36691179\n",
            "Iteration 1222, loss = 0.36637276\n",
            "Iteration 1223, loss = 0.36583435\n",
            "Iteration 1224, loss = 0.36529655\n",
            "Iteration 1225, loss = 0.36475937\n",
            "Iteration 1226, loss = 0.36422282\n",
            "Iteration 1227, loss = 0.36368689\n",
            "Iteration 1228, loss = 0.36315159\n",
            "Iteration 1229, loss = 0.36261691\n",
            "Iteration 1230, loss = 0.36208286\n",
            "Iteration 1231, loss = 0.36154942\n",
            "Iteration 1232, loss = 0.36101660\n",
            "Iteration 1233, loss = 0.36048441\n",
            "Iteration 1234, loss = 0.35995283\n",
            "Iteration 1235, loss = 0.35942188\n",
            "Iteration 1236, loss = 0.35889154\n",
            "Iteration 1237, loss = 0.35836183\n",
            "Iteration 1238, loss = 0.35783273\n",
            "Iteration 1239, loss = 0.35730425\n",
            "Iteration 1240, loss = 0.35677640\n",
            "Iteration 1241, loss = 0.35624916\n",
            "Iteration 1242, loss = 0.35572254\n",
            "Iteration 1243, loss = 0.35519655\n",
            "Iteration 1244, loss = 0.35467118\n",
            "Iteration 1245, loss = 0.35414642\n",
            "Iteration 1246, loss = 0.35362230\n",
            "Iteration 1247, loss = 0.35309879\n",
            "Iteration 1248, loss = 0.35257591\n",
            "Iteration 1249, loss = 0.35205366\n",
            "Iteration 1250, loss = 0.35153203\n",
            "Iteration 1251, loss = 0.35101103\n",
            "Iteration 1252, loss = 0.35049065\n",
            "Iteration 1253, loss = 0.34997091\n",
            "Iteration 1254, loss = 0.34945179\n",
            "Iteration 1255, loss = 0.34893331\n",
            "Iteration 1256, loss = 0.34841546\n",
            "Iteration 1257, loss = 0.34789824\n",
            "Iteration 1258, loss = 0.34738166\n",
            "Iteration 1259, loss = 0.34686570\n",
            "Iteration 1260, loss = 0.34635039\n",
            "Iteration 1261, loss = 0.34583571\n",
            "Iteration 1262, loss = 0.34532167\n",
            "Iteration 1263, loss = 0.34480827\n",
            "Iteration 1264, loss = 0.34429551\n",
            "Iteration 1265, loss = 0.34378340\n",
            "Iteration 1266, loss = 0.34327192\n",
            "Iteration 1267, loss = 0.34276109\n",
            "Iteration 1268, loss = 0.34225090\n",
            "Iteration 1269, loss = 0.34174136\n",
            "Iteration 1270, loss = 0.34123246\n",
            "Iteration 1271, loss = 0.34072422\n",
            "Iteration 1272, loss = 0.34021662\n",
            "Iteration 1273, loss = 0.33970968\n",
            "Iteration 1274, loss = 0.33920338\n",
            "Iteration 1275, loss = 0.33869774\n",
            "Iteration 1276, loss = 0.33819276\n",
            "Iteration 1277, loss = 0.33768843\n",
            "Iteration 1278, loss = 0.33718476\n",
            "Iteration 1279, loss = 0.33668175\n",
            "Iteration 1280, loss = 0.33617939\n",
            "Iteration 1281, loss = 0.33567770\n",
            "Iteration 1282, loss = 0.33517667\n",
            "Iteration 1283, loss = 0.33467631\n",
            "Iteration 1284, loss = 0.33417661\n",
            "Iteration 1285, loss = 0.33367757\n",
            "Iteration 1286, loss = 0.33317921\n",
            "Iteration 1287, loss = 0.33268151\n",
            "Iteration 1288, loss = 0.33218449\n",
            "Iteration 1289, loss = 0.33168814\n",
            "Iteration 1290, loss = 0.33119246\n",
            "Iteration 1291, loss = 0.33069746\n",
            "Iteration 1292, loss = 0.33020313\n",
            "Iteration 1293, loss = 0.32970948\n",
            "Iteration 1294, loss = 0.32921651\n",
            "Iteration 1295, loss = 0.32872422\n",
            "Iteration 1296, loss = 0.32823261\n",
            "Iteration 1297, loss = 0.32774169\n",
            "Iteration 1298, loss = 0.32725145\n",
            "Iteration 1299, loss = 0.32676190\n",
            "Iteration 1300, loss = 0.32627303\n",
            "Iteration 1301, loss = 0.32578486\n",
            "Iteration 1302, loss = 0.32529737\n",
            "Iteration 1303, loss = 0.32481057\n",
            "Iteration 1304, loss = 0.32432447\n",
            "Iteration 1305, loss = 0.32383907\n",
            "Iteration 1306, loss = 0.32335436\n",
            "Iteration 1307, loss = 0.32287034\n",
            "Iteration 1308, loss = 0.32238703\n",
            "Iteration 1309, loss = 0.32190441\n",
            "Iteration 1310, loss = 0.32142249\n",
            "Iteration 1311, loss = 0.32094128\n",
            "Iteration 1312, loss = 0.32046077\n",
            "Iteration 1313, loss = 0.31998097\n",
            "Iteration 1314, loss = 0.31950187\n",
            "Iteration 1315, loss = 0.31902348\n",
            "Iteration 1316, loss = 0.31854580\n",
            "Iteration 1317, loss = 0.31806883\n",
            "Iteration 1318, loss = 0.31759257\n",
            "Iteration 1319, loss = 0.31711703\n",
            "Iteration 1320, loss = 0.31664219\n",
            "Iteration 1321, loss = 0.31616808\n",
            "Iteration 1322, loss = 0.31569467\n",
            "Iteration 1323, loss = 0.31522199\n",
            "Iteration 1324, loss = 0.31475003\n",
            "Iteration 1325, loss = 0.31427878\n",
            "Iteration 1326, loss = 0.31380826\n",
            "Iteration 1327, loss = 0.31333846\n",
            "Iteration 1328, loss = 0.31286938\n",
            "Iteration 1329, loss = 0.31240103\n",
            "Iteration 1330, loss = 0.31193340\n",
            "Iteration 1331, loss = 0.31146650\n",
            "Iteration 1332, loss = 0.31100032\n",
            "Iteration 1333, loss = 0.31053488\n",
            "Iteration 1334, loss = 0.31007017\n",
            "Iteration 1335, loss = 0.30960618\n",
            "Iteration 1336, loss = 0.30914293\n",
            "Iteration 1337, loss = 0.30868041\n",
            "Iteration 1338, loss = 0.30821863\n",
            "Iteration 1339, loss = 0.30775758\n",
            "Iteration 1340, loss = 0.30729727\n",
            "Iteration 1341, loss = 0.30683770\n",
            "Iteration 1342, loss = 0.30637886\n",
            "Iteration 1343, loss = 0.30592076\n",
            "Iteration 1344, loss = 0.30546340\n",
            "Iteration 1345, loss = 0.30500679\n",
            "Iteration 1346, loss = 0.30455091\n",
            "Iteration 1347, loss = 0.30409578\n",
            "Iteration 1348, loss = 0.30364139\n",
            "Iteration 1349, loss = 0.30318775\n",
            "Iteration 1350, loss = 0.30273485\n",
            "Iteration 1351, loss = 0.30228269\n",
            "Iteration 1352, loss = 0.30183129\n",
            "Iteration 1353, loss = 0.30138063\n",
            "Iteration 1354, loss = 0.30093072\n",
            "Iteration 1355, loss = 0.30048155\n",
            "Iteration 1356, loss = 0.30003314\n",
            "Iteration 1357, loss = 0.29958548\n",
            "Iteration 1358, loss = 0.29913857\n",
            "Iteration 1359, loss = 0.29869241\n",
            "Iteration 1360, loss = 0.29824701\n",
            "Iteration 1361, loss = 0.29780236\n",
            "Iteration 1362, loss = 0.29735846\n",
            "Iteration 1363, loss = 0.29691532\n",
            "Iteration 1364, loss = 0.29647293\n",
            "Iteration 1365, loss = 0.29603130\n",
            "Iteration 1366, loss = 0.29559042\n",
            "Iteration 1367, loss = 0.29515030\n",
            "Iteration 1368, loss = 0.29471094\n",
            "Iteration 1369, loss = 0.29427234\n",
            "Iteration 1370, loss = 0.29383449\n",
            "Iteration 1371, loss = 0.29339741\n",
            "Iteration 1372, loss = 0.29296108\n",
            "Iteration 1373, loss = 0.29252551\n",
            "Iteration 1374, loss = 0.29209071\n",
            "Iteration 1375, loss = 0.29165666\n",
            "Iteration 1376, loss = 0.29122338\n",
            "Iteration 1377, loss = 0.29079087\n",
            "Iteration 1378, loss = 0.29035914\n",
            "Iteration 1379, loss = 0.28992816\n",
            "Iteration 1380, loss = 0.28949795\n",
            "Iteration 1381, loss = 0.28906851\n",
            "Iteration 1382, loss = 0.28863983\n",
            "Iteration 1383, loss = 0.28821191\n",
            "Iteration 1384, loss = 0.28778475\n",
            "Iteration 1385, loss = 0.28735836\n",
            "Iteration 1386, loss = 0.28693274\n",
            "Iteration 1387, loss = 0.28650788\n",
            "Iteration 1388, loss = 0.28608378\n",
            "Iteration 1389, loss = 0.28566045\n",
            "Iteration 1390, loss = 0.28523788\n",
            "Iteration 1391, loss = 0.28481608\n",
            "Iteration 1392, loss = 0.28439505\n",
            "Iteration 1393, loss = 0.28397478\n",
            "Iteration 1394, loss = 0.28355530\n",
            "Iteration 1395, loss = 0.28313671\n",
            "Iteration 1396, loss = 0.28271889\n",
            "Iteration 1397, loss = 0.28230186\n",
            "Iteration 1398, loss = 0.28188561\n",
            "Iteration 1399, loss = 0.28147015\n",
            "Iteration 1400, loss = 0.28105548\n",
            "Iteration 1401, loss = 0.28064158\n",
            "Iteration 1402, loss = 0.28022846\n",
            "Iteration 1403, loss = 0.27981610\n",
            "Iteration 1404, loss = 0.27940452\n",
            "Iteration 1405, loss = 0.27899371\n",
            "Iteration 1406, loss = 0.27858366\n",
            "Iteration 1407, loss = 0.27817438\n",
            "Iteration 1408, loss = 0.27776588\n",
            "Iteration 1409, loss = 0.27735814\n",
            "Iteration 1410, loss = 0.27695117\n",
            "Iteration 1411, loss = 0.27654497\n",
            "Iteration 1412, loss = 0.27613954\n",
            "Iteration 1413, loss = 0.27573488\n",
            "Iteration 1414, loss = 0.27533098\n",
            "Iteration 1415, loss = 0.27492786\n",
            "Iteration 1416, loss = 0.27452550\n",
            "Iteration 1417, loss = 0.27412390\n",
            "Iteration 1418, loss = 0.27372308\n",
            "Iteration 1419, loss = 0.27332301\n",
            "Iteration 1420, loss = 0.27292372\n",
            "Iteration 1421, loss = 0.27252519\n",
            "Iteration 1422, loss = 0.27212742\n",
            "Iteration 1423, loss = 0.27173042\n",
            "Iteration 1424, loss = 0.27133418\n",
            "Iteration 1425, loss = 0.27093870\n",
            "Iteration 1426, loss = 0.27054399\n",
            "Iteration 1427, loss = 0.27015004\n",
            "Iteration 1428, loss = 0.26975686\n",
            "Iteration 1429, loss = 0.26936443\n",
            "Iteration 1430, loss = 0.26897277\n",
            "Iteration 1431, loss = 0.26858187\n",
            "Iteration 1432, loss = 0.26819172\n",
            "Iteration 1433, loss = 0.26780234\n",
            "Iteration 1434, loss = 0.26741372\n",
            "Iteration 1435, loss = 0.26702586\n",
            "Iteration 1436, loss = 0.26663875\n",
            "Iteration 1437, loss = 0.26625241\n",
            "Iteration 1438, loss = 0.26586682\n",
            "Iteration 1439, loss = 0.26548199\n",
            "Iteration 1440, loss = 0.26509791\n",
            "Iteration 1441, loss = 0.26471460\n",
            "Iteration 1442, loss = 0.26433203\n",
            "Iteration 1443, loss = 0.26395023\n",
            "Iteration 1444, loss = 0.26356917\n",
            "Iteration 1445, loss = 0.26318887\n",
            "Iteration 1446, loss = 0.26280933\n",
            "Iteration 1447, loss = 0.26243053\n",
            "Iteration 1448, loss = 0.26205249\n",
            "Iteration 1449, loss = 0.26167520\n",
            "Iteration 1450, loss = 0.26129866\n",
            "Iteration 1451, loss = 0.26092288\n",
            "Iteration 1452, loss = 0.26054784\n",
            "Iteration 1453, loss = 0.26017355\n",
            "Iteration 1454, loss = 0.25980000\n",
            "Iteration 1455, loss = 0.25942721\n",
            "Iteration 1456, loss = 0.25905516\n",
            "Iteration 1457, loss = 0.25868502\n",
            "Iteration 1458, loss = 0.25831507\n",
            "Iteration 1459, loss = 0.25794582\n",
            "Iteration 1460, loss = 0.25757743\n",
            "Iteration 1461, loss = 0.25720991\n",
            "Iteration 1462, loss = 0.25684370\n",
            "Iteration 1463, loss = 0.25647736\n",
            "Iteration 1464, loss = 0.25611213\n",
            "Iteration 1465, loss = 0.25574763\n",
            "Iteration 1466, loss = 0.25538411\n",
            "Iteration 1467, loss = 0.25502123\n",
            "Iteration 1468, loss = 0.25465918\n",
            "Iteration 1469, loss = 0.25429785\n",
            "Iteration 1470, loss = 0.25393732\n",
            "Iteration 1471, loss = 0.25357784\n",
            "Iteration 1472, loss = 0.25321886\n",
            "Iteration 1473, loss = 0.25286065\n",
            "Iteration 1474, loss = 0.25250349\n",
            "Iteration 1475, loss = 0.25214682\n",
            "Iteration 1476, loss = 0.25179089\n",
            "Iteration 1477, loss = 0.25143606\n",
            "Iteration 1478, loss = 0.25108161\n",
            "Iteration 1479, loss = 0.25072804\n",
            "Iteration 1480, loss = 0.25037522\n",
            "Iteration 1481, loss = 0.25002317\n",
            "Iteration 1482, loss = 0.24967187\n",
            "Iteration 1483, loss = 0.24932135\n",
            "Iteration 1484, loss = 0.24897159\n",
            "Iteration 1485, loss = 0.24862276\n",
            "Iteration 1486, loss = 0.24827495\n",
            "Iteration 1487, loss = 0.24792788\n",
            "Iteration 1488, loss = 0.24758161\n",
            "Iteration 1489, loss = 0.24723619\n",
            "Iteration 1490, loss = 0.24689162\n",
            "Iteration 1491, loss = 0.24654788\n",
            "Iteration 1492, loss = 0.24620494\n",
            "Iteration 1493, loss = 0.24586278\n",
            "Iteration 1494, loss = 0.24552137\n",
            "Iteration 1495, loss = 0.24518067\n",
            "Iteration 1496, loss = 0.24484069\n",
            "Iteration 1497, loss = 0.24450141\n",
            "Iteration 1498, loss = 0.24416285\n",
            "Iteration 1499, loss = 0.24382500\n",
            "Iteration 1500, loss = 0.24348787\n",
            "Iteration 1501, loss = 0.24315146\n",
            "Iteration 1502, loss = 0.24281579\n",
            "Iteration 1503, loss = 0.24248083\n",
            "Iteration 1504, loss = 0.24214660\n",
            "Iteration 1505, loss = 0.24181308\n",
            "Iteration 1506, loss = 0.24148026\n",
            "Iteration 1507, loss = 0.24114815\n",
            "Iteration 1508, loss = 0.24081673\n",
            "Iteration 1509, loss = 0.24048602\n",
            "Iteration 1510, loss = 0.24015599\n",
            "Iteration 1511, loss = 0.23982666\n",
            "Iteration 1512, loss = 0.23949803\n",
            "Iteration 1513, loss = 0.23917008\n",
            "Iteration 1514, loss = 0.23884283\n",
            "Iteration 1515, loss = 0.23851627\n",
            "Iteration 1516, loss = 0.23819039\n",
            "Iteration 1517, loss = 0.23786520\n",
            "Iteration 1518, loss = 0.23754070\n",
            "Iteration 1519, loss = 0.23721688\n",
            "Iteration 1520, loss = 0.23689373\n",
            "Iteration 1521, loss = 0.23657127\n",
            "Iteration 1522, loss = 0.23624948\n",
            "Iteration 1523, loss = 0.23592837\n",
            "Iteration 1524, loss = 0.23560793\n",
            "Iteration 1525, loss = 0.23528817\n",
            "Iteration 1526, loss = 0.23496908\n",
            "Iteration 1527, loss = 0.23465066\n",
            "Iteration 1528, loss = 0.23433291\n",
            "Iteration 1529, loss = 0.23401582\n",
            "Iteration 1530, loss = 0.23369940\n",
            "Iteration 1531, loss = 0.23338364\n",
            "Iteration 1532, loss = 0.23306855\n",
            "Iteration 1533, loss = 0.23275412\n",
            "Iteration 1534, loss = 0.23244035\n",
            "Iteration 1535, loss = 0.23212723\n",
            "Iteration 1536, loss = 0.23181478\n",
            "Iteration 1537, loss = 0.23150298\n",
            "Iteration 1538, loss = 0.23119183\n",
            "Iteration 1539, loss = 0.23088133\n",
            "Iteration 1540, loss = 0.23057149\n",
            "Iteration 1541, loss = 0.23026229\n",
            "Iteration 1542, loss = 0.22995375\n",
            "Iteration 1543, loss = 0.22964585\n",
            "Iteration 1544, loss = 0.22933860\n",
            "Iteration 1545, loss = 0.22903199\n",
            "Iteration 1546, loss = 0.22872602\n",
            "Iteration 1547, loss = 0.22842070\n",
            "Iteration 1548, loss = 0.22811601\n",
            "Iteration 1549, loss = 0.22781197\n",
            "Iteration 1550, loss = 0.22750856\n",
            "Iteration 1551, loss = 0.22720579\n",
            "Iteration 1552, loss = 0.22690365\n",
            "Iteration 1553, loss = 0.22660214\n",
            "Iteration 1554, loss = 0.22630127\n",
            "Iteration 1555, loss = 0.22600103\n",
            "Iteration 1556, loss = 0.22570142\n",
            "Iteration 1557, loss = 0.22540243\n",
            "Iteration 1558, loss = 0.22510407\n",
            "Iteration 1559, loss = 0.22480634\n",
            "Iteration 1560, loss = 0.22450923\n",
            "Iteration 1561, loss = 0.22421275\n",
            "Iteration 1562, loss = 0.22391688\n",
            "Iteration 1563, loss = 0.22362163\n",
            "Iteration 1564, loss = 0.22332701\n",
            "Iteration 1565, loss = 0.22303300\n",
            "Iteration 1566, loss = 0.22273960\n",
            "Iteration 1567, loss = 0.22244682\n",
            "Iteration 1568, loss = 0.22215466\n",
            "Iteration 1569, loss = 0.22186310\n",
            "Iteration 1570, loss = 0.22157216\n",
            "Iteration 1571, loss = 0.22128183\n",
            "Iteration 1572, loss = 0.22099210\n",
            "Iteration 1573, loss = 0.22070298\n",
            "Iteration 1574, loss = 0.22041447\n",
            "Iteration 1575, loss = 0.22012656\n",
            "Iteration 1576, loss = 0.21983925\n",
            "Iteration 1577, loss = 0.21955314\n",
            "Iteration 1578, loss = 0.21926725\n",
            "Iteration 1579, loss = 0.21898195\n",
            "Iteration 1580, loss = 0.21869732\n",
            "Iteration 1581, loss = 0.21841357\n",
            "Iteration 1582, loss = 0.21813016\n",
            "Iteration 1583, loss = 0.21784744\n",
            "Iteration 1584, loss = 0.21756535\n",
            "Iteration 1585, loss = 0.21728391\n",
            "Iteration 1586, loss = 0.21700302\n",
            "Iteration 1587, loss = 0.21672272\n",
            "Iteration 1588, loss = 0.21644302\n",
            "Iteration 1589, loss = 0.21616395\n",
            "Iteration 1590, loss = 0.21588561\n",
            "Iteration 1591, loss = 0.21560778\n",
            "Iteration 1592, loss = 0.21533053\n",
            "Iteration 1593, loss = 0.21505410\n",
            "Iteration 1594, loss = 0.21477800\n",
            "Iteration 1595, loss = 0.21450260\n",
            "Iteration 1596, loss = 0.21422786\n",
            "Iteration 1597, loss = 0.21395370\n",
            "Iteration 1598, loss = 0.21368009\n",
            "Iteration 1599, loss = 0.21340706\n",
            "Iteration 1600, loss = 0.21313463\n",
            "Iteration 1601, loss = 0.21286280\n",
            "Iteration 1602, loss = 0.21259162\n",
            "Iteration 1603, loss = 0.21232095\n",
            "Iteration 1604, loss = 0.21205089\n",
            "Iteration 1605, loss = 0.21178140\n",
            "Iteration 1606, loss = 0.21151249\n",
            "Iteration 1607, loss = 0.21124415\n",
            "Iteration 1608, loss = 0.21097639\n",
            "Iteration 1609, loss = 0.21070920\n",
            "Iteration 1610, loss = 0.21044259\n",
            "Iteration 1611, loss = 0.21017654\n",
            "Iteration 1612, loss = 0.20991106\n",
            "Iteration 1613, loss = 0.20964614\n",
            "Iteration 1614, loss = 0.20938179\n",
            "Iteration 1615, loss = 0.20911799\n",
            "Iteration 1616, loss = 0.20885475\n",
            "Iteration 1617, loss = 0.20859207\n",
            "Iteration 1618, loss = 0.20832994\n",
            "Iteration 1619, loss = 0.20806836\n",
            "Iteration 1620, loss = 0.20780734\n",
            "Iteration 1621, loss = 0.20754686\n",
            "Iteration 1622, loss = 0.20728693\n",
            "Iteration 1623, loss = 0.20702754\n",
            "Iteration 1624, loss = 0.20676870\n",
            "Iteration 1625, loss = 0.20651040\n",
            "Iteration 1626, loss = 0.20625263\n",
            "Iteration 1627, loss = 0.20599541\n",
            "Iteration 1628, loss = 0.20573872\n",
            "Iteration 1629, loss = 0.20548256\n",
            "Iteration 1630, loss = 0.20522694\n",
            "Iteration 1631, loss = 0.20497185\n",
            "Iteration 1632, loss = 0.20471730\n",
            "Iteration 1633, loss = 0.20446327\n",
            "Iteration 1634, loss = 0.20420976\n",
            "Iteration 1635, loss = 0.20395679\n",
            "Iteration 1636, loss = 0.20370433\n",
            "Iteration 1637, loss = 0.20345241\n",
            "Iteration 1638, loss = 0.20320100\n",
            "Iteration 1639, loss = 0.20295011\n",
            "Iteration 1640, loss = 0.20269974\n",
            "Iteration 1641, loss = 0.20244989\n",
            "Iteration 1642, loss = 0.20220087\n",
            "Iteration 1643, loss = 0.20195223\n",
            "Iteration 1644, loss = 0.20170411\n",
            "Iteration 1645, loss = 0.20145655\n",
            "Iteration 1646, loss = 0.20120955\n",
            "Iteration 1647, loss = 0.20096310\n",
            "Iteration 1648, loss = 0.20071716\n",
            "Iteration 1649, loss = 0.20047173\n",
            "Iteration 1650, loss = 0.20022680\n",
            "Iteration 1651, loss = 0.19998239\n",
            "Iteration 1652, loss = 0.19973851\n",
            "Iteration 1653, loss = 0.19949515\n",
            "Iteration 1654, loss = 0.19925231\n",
            "Iteration 1655, loss = 0.19900999\n",
            "Iteration 1656, loss = 0.19876820\n",
            "Iteration 1657, loss = 0.19852694\n",
            "Iteration 1658, loss = 0.19828619\n",
            "Iteration 1659, loss = 0.19804599\n",
            "Iteration 1660, loss = 0.19780629\n",
            "Iteration 1661, loss = 0.19756708\n",
            "Iteration 1662, loss = 0.19732838\n",
            "Iteration 1663, loss = 0.19709019\n",
            "Iteration 1664, loss = 0.19685250\n",
            "Iteration 1665, loss = 0.19661533\n",
            "Iteration 1666, loss = 0.19637866\n",
            "Iteration 1667, loss = 0.19614249\n",
            "Iteration 1668, loss = 0.19590681\n",
            "Iteration 1669, loss = 0.19567163\n",
            "Iteration 1670, loss = 0.19543693\n",
            "Iteration 1671, loss = 0.19520273\n",
            "Iteration 1672, loss = 0.19496901\n",
            "Iteration 1673, loss = 0.19473578\n",
            "Iteration 1674, loss = 0.19450303\n",
            "Iteration 1675, loss = 0.19427076\n",
            "Iteration 1676, loss = 0.19403898\n",
            "Iteration 1677, loss = 0.19380766\n",
            "Iteration 1678, loss = 0.19357683\n",
            "Iteration 1679, loss = 0.19334646\n",
            "Iteration 1680, loss = 0.19311657\n",
            "Iteration 1681, loss = 0.19288714\n",
            "Iteration 1682, loss = 0.19265819\n",
            "Iteration 1683, loss = 0.19242970\n",
            "Iteration 1684, loss = 0.19220167\n",
            "Iteration 1685, loss = 0.19197411\n",
            "Iteration 1686, loss = 0.19174701\n",
            "Iteration 1687, loss = 0.19152038\n",
            "Iteration 1688, loss = 0.19129420\n",
            "Iteration 1689, loss = 0.19106848\n",
            "Iteration 1690, loss = 0.19084321\n",
            "Iteration 1691, loss = 0.19061841\n",
            "Iteration 1692, loss = 0.19039405\n",
            "Iteration 1693, loss = 0.19017015\n",
            "Iteration 1694, loss = 0.18994670\n",
            "Iteration 1695, loss = 0.18972370\n",
            "Iteration 1696, loss = 0.18950115\n",
            "Iteration 1697, loss = 0.18927904\n",
            "Iteration 1698, loss = 0.18905738\n",
            "Iteration 1699, loss = 0.18883617\n",
            "Iteration 1700, loss = 0.18861540\n",
            "Iteration 1701, loss = 0.18839507\n",
            "Iteration 1702, loss = 0.18817519\n",
            "Iteration 1703, loss = 0.18795574\n",
            "Iteration 1704, loss = 0.18773673\n",
            "Iteration 1705, loss = 0.18751816\n",
            "Iteration 1706, loss = 0.18730003\n",
            "Iteration 1707, loss = 0.18708233\n",
            "Iteration 1708, loss = 0.18686507\n",
            "Iteration 1709, loss = 0.18664825\n",
            "Iteration 1710, loss = 0.18643186\n",
            "Iteration 1711, loss = 0.18621590\n",
            "Iteration 1712, loss = 0.18600037\n",
            "Iteration 1713, loss = 0.18578526\n",
            "Iteration 1714, loss = 0.18557058\n",
            "Iteration 1715, loss = 0.18535633\n",
            "Iteration 1716, loss = 0.18514250\n",
            "Iteration 1717, loss = 0.18492910\n",
            "Iteration 1718, loss = 0.18471611\n",
            "Iteration 1719, loss = 0.18450355\n",
            "Iteration 1720, loss = 0.18429141\n",
            "Iteration 1721, loss = 0.18407968\n",
            "Iteration 1722, loss = 0.18386837\n",
            "Iteration 1723, loss = 0.18365748\n",
            "Iteration 1724, loss = 0.18344700\n",
            "Iteration 1725, loss = 0.18323693\n",
            "Iteration 1726, loss = 0.18302728\n",
            "Iteration 1727, loss = 0.18281804\n",
            "Iteration 1728, loss = 0.18260921\n",
            "Iteration 1729, loss = 0.18240079\n",
            "Iteration 1730, loss = 0.18219278\n",
            "Iteration 1731, loss = 0.18198517\n",
            "Iteration 1732, loss = 0.18177797\n",
            "Iteration 1733, loss = 0.18157118\n",
            "Iteration 1734, loss = 0.18136479\n",
            "Iteration 1735, loss = 0.18115880\n",
            "Iteration 1736, loss = 0.18095321\n",
            "Iteration 1737, loss = 0.18074803\n",
            "Iteration 1738, loss = 0.18054324\n",
            "Iteration 1739, loss = 0.18033885\n",
            "Iteration 1740, loss = 0.18013486\n",
            "Iteration 1741, loss = 0.17993127\n",
            "Iteration 1742, loss = 0.17972807\n",
            "Iteration 1743, loss = 0.17952527\n",
            "Iteration 1744, loss = 0.17932286\n",
            "Iteration 1745, loss = 0.17912084\n",
            "Iteration 1746, loss = 0.17891922\n",
            "Iteration 1747, loss = 0.17871798\n",
            "Iteration 1748, loss = 0.17851714\n",
            "Iteration 1749, loss = 0.17831668\n",
            "Iteration 1750, loss = 0.17811661\n",
            "Iteration 1751, loss = 0.17791693\n",
            "Iteration 1752, loss = 0.17771763\n",
            "Iteration 1753, loss = 0.17751871\n",
            "Iteration 1754, loss = 0.17732018\n",
            "Iteration 1755, loss = 0.17712203\n",
            "Iteration 1756, loss = 0.17692427\n",
            "Iteration 1757, loss = 0.17672688\n",
            "Iteration 1758, loss = 0.17652987\n",
            "Iteration 1759, loss = 0.17633325\n",
            "Iteration 1760, loss = 0.17613699\n",
            "Iteration 1761, loss = 0.17594112\n",
            "Iteration 1762, loss = 0.17574562\n",
            "Iteration 1763, loss = 0.17555050\n",
            "Iteration 1764, loss = 0.17535575\n",
            "Iteration 1765, loss = 0.17516137\n",
            "Iteration 1766, loss = 0.17496737\n",
            "Iteration 1767, loss = 0.17477373\n",
            "Iteration 1768, loss = 0.17458047\n",
            "Iteration 1769, loss = 0.17438757\n",
            "Iteration 1770, loss = 0.17419504\n",
            "Iteration 1771, loss = 0.17400288\n",
            "Iteration 1772, loss = 0.17381109\n",
            "Iteration 1773, loss = 0.17361966\n",
            "Iteration 1774, loss = 0.17342860\n",
            "Iteration 1775, loss = 0.17323789\n",
            "Iteration 1776, loss = 0.17304756\n",
            "Iteration 1777, loss = 0.17285758\n",
            "Iteration 1778, loss = 0.17266796\n",
            "Iteration 1779, loss = 0.17247871\n",
            "Iteration 1780, loss = 0.17228981\n",
            "Iteration 1781, loss = 0.17210127\n",
            "Iteration 1782, loss = 0.17191309\n",
            "Iteration 1783, loss = 0.17172526\n",
            "Iteration 1784, loss = 0.17153779\n",
            "Iteration 1785, loss = 0.17135067\n",
            "Iteration 1786, loss = 0.17116391\n",
            "Iteration 1787, loss = 0.17097750\n",
            "Iteration 1788, loss = 0.17079144\n",
            "Iteration 1789, loss = 0.17060573\n",
            "Iteration 1790, loss = 0.17042038\n",
            "Iteration 1791, loss = 0.17023537\n",
            "Iteration 1792, loss = 0.17005071\n",
            "Iteration 1793, loss = 0.16986640\n",
            "Iteration 1794, loss = 0.16968243\n",
            "Iteration 1795, loss = 0.16949881\n",
            "Iteration 1796, loss = 0.16931553\n",
            "Iteration 1797, loss = 0.16913260\n",
            "Iteration 1798, loss = 0.16895001\n",
            "Iteration 1799, loss = 0.16876776\n",
            "Iteration 1800, loss = 0.16858586\n",
            "Iteration 1801, loss = 0.16840429\n",
            "Iteration 1802, loss = 0.16822307\n",
            "Iteration 1803, loss = 0.16804218\n",
            "Iteration 1804, loss = 0.16786163\n",
            "Iteration 1805, loss = 0.16768142\n",
            "Iteration 1806, loss = 0.16750155\n",
            "Iteration 1807, loss = 0.16732201\n",
            "Iteration 1808, loss = 0.16714280\n",
            "Iteration 1809, loss = 0.16696393\n",
            "Iteration 1810, loss = 0.16678539\n",
            "Iteration 1811, loss = 0.16660719\n",
            "Iteration 1812, loss = 0.16642931\n",
            "Iteration 1813, loss = 0.16625177\n",
            "Iteration 1814, loss = 0.16607455\n",
            "Iteration 1815, loss = 0.16589767\n",
            "Iteration 1816, loss = 0.16572111\n",
            "Iteration 1817, loss = 0.16554488\n",
            "Iteration 1818, loss = 0.16536898\n",
            "Iteration 1819, loss = 0.16519340\n",
            "Iteration 1820, loss = 0.16501814\n",
            "Iteration 1821, loss = 0.16484321\n",
            "Iteration 1822, loss = 0.16466861\n",
            "Iteration 1823, loss = 0.16449432\n",
            "Iteration 1824, loss = 0.16432036\n",
            "Iteration 1825, loss = 0.16414672\n",
            "Iteration 1826, loss = 0.16397339\n",
            "Iteration 1827, loss = 0.16380039\n",
            "Iteration 1828, loss = 0.16362771\n",
            "Iteration 1829, loss = 0.16345534\n",
            "Iteration 1830, loss = 0.16328329\n",
            "Iteration 1831, loss = 0.16311156\n",
            "Iteration 1832, loss = 0.16294014\n",
            "Iteration 1833, loss = 0.16276903\n",
            "Iteration 1834, loss = 0.16259824\n",
            "Iteration 1835, loss = 0.16242776\n",
            "Iteration 1836, loss = 0.16225760\n",
            "Iteration 1837, loss = 0.16208774\n",
            "Iteration 1838, loss = 0.16191820\n",
            "Iteration 1839, loss = 0.16174897\n",
            "Iteration 1840, loss = 0.16158004\n",
            "Iteration 1841, loss = 0.16141143\n",
            "Iteration 1842, loss = 0.16124312\n",
            "Iteration 1843, loss = 0.16107511\n",
            "Iteration 1844, loss = 0.16090742\n",
            "Iteration 1845, loss = 0.16074003\n",
            "Iteration 1846, loss = 0.16057294\n",
            "Iteration 1847, loss = 0.16040616\n",
            "Iteration 1848, loss = 0.16023968\n",
            "Iteration 1849, loss = 0.16007350\n",
            "Iteration 1850, loss = 0.15990763\n",
            "Iteration 1851, loss = 0.15974205\n",
            "Iteration 1852, loss = 0.15957678\n",
            "Iteration 1853, loss = 0.15941180\n",
            "Iteration 1854, loss = 0.15924712\n",
            "Iteration 1855, loss = 0.15908275\n",
            "Iteration 1856, loss = 0.15891866\n",
            "Iteration 1857, loss = 0.15875488\n",
            "Iteration 1858, loss = 0.15859139\n",
            "Iteration 1859, loss = 0.15842819\n",
            "Iteration 1860, loss = 0.15826529\n",
            "Iteration 1861, loss = 0.15810268\n",
            "Iteration 1862, loss = 0.15794037\n",
            "Iteration 1863, loss = 0.15777835\n",
            "Iteration 1864, loss = 0.15761662\n",
            "Iteration 1865, loss = 0.15745517\n",
            "Iteration 1866, loss = 0.15729402\n",
            "Iteration 1867, loss = 0.15713316\n",
            "Iteration 1868, loss = 0.15697259\n",
            "Iteration 1869, loss = 0.15681230\n",
            "Iteration 1870, loss = 0.15665230\n",
            "Iteration 1871, loss = 0.15649259\n",
            "Iteration 1872, loss = 0.15633317\n",
            "Iteration 1873, loss = 0.15617402\n",
            "Iteration 1874, loss = 0.15601517\n",
            "Iteration 1875, loss = 0.15585659\n",
            "Iteration 1876, loss = 0.15569830\n",
            "Iteration 1877, loss = 0.15554029\n",
            "Iteration 1878, loss = 0.15538256\n",
            "Iteration 1879, loss = 0.15522512\n",
            "Iteration 1880, loss = 0.15506795\n",
            "Iteration 1881, loss = 0.15491106\n",
            "Iteration 1882, loss = 0.15475446\n",
            "Iteration 1883, loss = 0.15459812\n",
            "Iteration 1884, loss = 0.15444207\n",
            "Iteration 1885, loss = 0.15428629\n",
            "Iteration 1886, loss = 0.15413079\n",
            "Iteration 1887, loss = 0.15397557\n",
            "Iteration 1888, loss = 0.15382062\n",
            "Iteration 1889, loss = 0.15366594\n",
            "Iteration 1890, loss = 0.15351154\n",
            "Iteration 1891, loss = 0.15335741\n",
            "Iteration 1892, loss = 0.15320355\n",
            "Iteration 1893, loss = 0.15304996\n",
            "Iteration 1894, loss = 0.15289664\n",
            "Iteration 1895, loss = 0.15274360\n",
            "Iteration 1896, loss = 0.15259082\n",
            "Iteration 1897, loss = 0.15243831\n",
            "Iteration 1898, loss = 0.15228607\n",
            "Iteration 1899, loss = 0.15213409\n",
            "Iteration 1900, loss = 0.15198239\n",
            "Iteration 1901, loss = 0.15183094\n",
            "Iteration 1902, loss = 0.15167977\n",
            "Iteration 1903, loss = 0.15152886\n",
            "Iteration 1904, loss = 0.15137821\n",
            "Iteration 1905, loss = 0.15122783\n",
            "Iteration 1906, loss = 0.15107771\n",
            "Iteration 1907, loss = 0.15092785\n",
            "Iteration 1908, loss = 0.15077825\n",
            "Iteration 1909, loss = 0.15062891\n",
            "Iteration 1910, loss = 0.15047984\n",
            "Iteration 1911, loss = 0.15033102\n",
            "Iteration 1912, loss = 0.15018246\n",
            "Iteration 1913, loss = 0.15003417\n",
            "Iteration 1914, loss = 0.14988612\n",
            "Iteration 1915, loss = 0.14973834\n",
            "Iteration 1916, loss = 0.14959081\n",
            "Iteration 1917, loss = 0.14944354\n",
            "Iteration 1918, loss = 0.14929652\n",
            "Iteration 1919, loss = 0.14914976\n",
            "Iteration 1920, loss = 0.14900325\n",
            "Iteration 1921, loss = 0.14885700\n",
            "Iteration 1922, loss = 0.14871100\n",
            "Iteration 1923, loss = 0.14856525\n",
            "Iteration 1924, loss = 0.14841975\n",
            "Iteration 1925, loss = 0.14827450\n",
            "Iteration 1926, loss = 0.14812950\n",
            "Iteration 1927, loss = 0.14798476\n",
            "Iteration 1928, loss = 0.14784026\n",
            "Iteration 1929, loss = 0.14769601\n",
            "Iteration 1930, loss = 0.14755201\n",
            "Iteration 1931, loss = 0.14740825\n",
            "Iteration 1932, loss = 0.14726474\n",
            "Iteration 1933, loss = 0.14712148\n",
            "Iteration 1934, loss = 0.14697847\n",
            "Iteration 1935, loss = 0.14683569\n",
            "Iteration 1936, loss = 0.14669317\n",
            "Iteration 1937, loss = 0.14655088\n",
            "Iteration 1938, loss = 0.14640884\n",
            "Iteration 1939, loss = 0.14626705\n",
            "Iteration 1940, loss = 0.14612549\n",
            "Iteration 1941, loss = 0.14598418\n",
            "Iteration 1942, loss = 0.14584310\n",
            "Iteration 1943, loss = 0.14570227\n",
            "Iteration 1944, loss = 0.14556167\n",
            "Iteration 1945, loss = 0.14542132\n",
            "Iteration 1946, loss = 0.14528120\n",
            "Iteration 1947, loss = 0.14514133\n",
            "Iteration 1948, loss = 0.14500168\n",
            "Iteration 1949, loss = 0.14486228\n",
            "Iteration 1950, loss = 0.14472311\n",
            "Iteration 1951, loss = 0.14458418\n",
            "Iteration 1952, loss = 0.14444548\n",
            "Iteration 1953, loss = 0.14430702\n",
            "Iteration 1954, loss = 0.14416879\n",
            "Iteration 1955, loss = 0.14403079\n",
            "Iteration 1956, loss = 0.14389303\n",
            "Iteration 1957, loss = 0.14375550\n",
            "Iteration 1958, loss = 0.14361820\n",
            "Iteration 1959, loss = 0.14348114\n",
            "Iteration 1960, loss = 0.14334430\n",
            "Iteration 1961, loss = 0.14320769\n",
            "Iteration 1962, loss = 0.14307131\n",
            "Iteration 1963, loss = 0.14293517\n",
            "Iteration 1964, loss = 0.14279924\n",
            "Iteration 1965, loss = 0.14266355\n",
            "Iteration 1966, loss = 0.14252809\n",
            "Iteration 1967, loss = 0.14239285\n",
            "Iteration 1968, loss = 0.14225784\n",
            "Iteration 1969, loss = 0.14212305\n",
            "Iteration 1970, loss = 0.14198849\n",
            "Iteration 1971, loss = 0.14185415\n",
            "Iteration 1972, loss = 0.14172004\n",
            "Iteration 1973, loss = 0.14158615\n",
            "Iteration 1974, loss = 0.14145248\n",
            "Iteration 1975, loss = 0.14131904\n",
            "Iteration 1976, loss = 0.14118582\n",
            "Iteration 1977, loss = 0.14105282\n",
            "Iteration 1978, loss = 0.14092004\n",
            "Iteration 1979, loss = 0.14078799\n",
            "Iteration 1980, loss = 0.14065609\n",
            "Iteration 1981, loss = 0.14052444\n",
            "Iteration 1982, loss = 0.14039305\n",
            "Iteration 1983, loss = 0.14026193\n",
            "Iteration 1984, loss = 0.14013106\n",
            "Iteration 1985, loss = 0.14000042\n",
            "Iteration 1986, loss = 0.13986999\n",
            "Iteration 1987, loss = 0.13973978\n",
            "Iteration 1988, loss = 0.13960980\n",
            "Iteration 1989, loss = 0.13948004\n",
            "Iteration 1990, loss = 0.13935052\n",
            "Iteration 1991, loss = 0.13922123\n",
            "Iteration 1992, loss = 0.13909217\n",
            "Iteration 1993, loss = 0.13896334\n",
            "Iteration 1994, loss = 0.13883473\n",
            "Iteration 1995, loss = 0.13870635\n",
            "Iteration 1996, loss = 0.13857819\n",
            "Iteration 1997, loss = 0.13845025\n",
            "Iteration 1998, loss = 0.13832254\n",
            "Iteration 1999, loss = 0.13819504\n",
            "Iteration 2000, loss = 0.13806777\n",
            "[[1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdjaRv01G6XY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9919c0a2-1c43-4971-ec7c-162a47568f07"
      },
      "source": [
        "#scikit for machine learning reporting\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test,y_pred)) # Print summary report\n",
        "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))) # Print Confusion matrix \n",
        "print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      0.93      0.97        15\n",
            "           2       0.83      1.00      0.91        15\n",
            "\n",
            "   micro avg       0.94      0.98      0.96        45\n",
            "   macro avg       0.94      0.98      0.96        45\n",
            "weighted avg       0.94      0.98      0.96        45\n",
            " samples avg       0.96      0.98      0.96        45\n",
            "\n",
            "[[15  0  0]\n",
            " [ 0 14  1]\n",
            " [ 0  0 15]]\n",
            "accuracy is  0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8wMCbNJG-h3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "fb2912c0-c672-4359-9184-55d4228fcf62"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(h.loss_curve_)\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Loss'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9f2e9f0240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD9bFcA4eU4quAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}